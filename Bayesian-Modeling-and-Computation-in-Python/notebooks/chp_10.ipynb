{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(chap10)=\n",
    "\n",
    "# 10. Probabilistic Programming Languages\n",
    "\n",
    "In Chapter [1](chap1) Section {ref}`bayesian_modeling`, we used cars as analogy to\n",
    "understand applied Bayesian concepts. We will revisit this analogy but\n",
    "this time to understand Probabilistic Programming Languages. If we treat\n",
    "a car as a system, their purpose is to move people or cargo to a chosen\n",
    "destination with wheels connected to a power source. This whole system\n",
    "is presented to users with an interface, typically a steering wheel and\n",
    "pedals. Like all physical objects cars have to obey the laws of physics,\n",
    "but within those bounds human designers have a multitude of components\n",
    "to choose from. Cars can have big engines, or small tires, 1 seat or 8\n",
    "seats. The end result however, is informed by the specific purpose. Some\n",
    "cars are designed to go fast with a single person around a racetrack,\n",
    "such as Formula 1 cars. Others are designed for family life like\n",
    "carrying families and groceries home from the store. No matter what the\n",
    "purpose, someone needs to pick the components, for the right car, for\n",
    "the right purpose.\n",
    "\n",
    "The story of Probabilistic Programming Languages (PPL) is similar. The\n",
    "purpose of PPLs is help Bayesian practitioners build generative models\n",
    "to solve problem at hand, for example, perform inference of a Bayesian\n",
    "model through estimating the posterior distribution with MCMC. The power\n",
    "source for computational Bayesians is, well, a computer which are bound\n",
    "by computer science fundamentals. Within those bounds however, PPL\n",
    "designers can choose different components and interfaces, with the\n",
    "specifics being determined by anticipated user need and preference. In\n",
    "this chapter we will focus our discussion on what the components of PPLs\n",
    "are, and different design choices that can be made within those\n",
    "components. This knowledge will help you as Bayesian practitioner when\n",
    "you need to pick a PPL when starting a project or debug that arises\n",
    "during your statistical workflow. This understanding ultimately will\n",
    "lead to better experience for you, the modern Bayesian practitioner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a-systems-engineering-perspective-of-a-ppl)=\n",
    "\n",
    "## 10.1. A Systems Engineering Perspective of a PPL\n",
    "\n",
    "Wikipedia defines systems engineering as \"an interdisciplinary field of\n",
    "engineering and engineering management that focuses on how to design,\n",
    "integrate, and manage complex systems over their life cycles\\\". PPLs are\n",
    "by this definition a complex system. PPLs span across computational\n",
    "backends, algorithms, and base languages. As the definition also states\n",
    "the integration of components is a key part of systems engineering,\n",
    "which is true of PPLs as well. A choice of computational backend can\n",
    "have an effect on the interface, or the choice of base language can\n",
    "limit the available inference algorithms. In some PPLs the user can make\n",
    "component selections themselves. For example, Stan users can choose\n",
    "their base language between R, Python, or the command line interface\n",
    "among others, whereas PyMC3 users cannot change base languages, they\n",
    "must use Python.\n",
    "\n",
    "In addition to the PPL itself there is the consideration of the\n",
    "organization and manner in which it will be used. A PhD student using a\n",
    "PPL in a research lab has different needs than an engineer working in a\n",
    "corporation. This is also relevant to the lifecycle of a PPL.\n",
    "Researchers may only need a model once or twice in a short period to\n",
    "write a paper, whereas an engineer in a corporation may maintain and run\n",
    "the model over the course of years.\n",
    "\n",
    "In a PPL the two necessary components are: an application programming\n",
    "interface for the user to define the model [^1], and algorithms to\n",
    "performs inference and manage the computation. Other components exist\n",
    "but largely to improve the system in some fashion, such as computational\n",
    "speed or ease of use. Regardless of the choice components, when a system\n",
    "is designed well the day to day user need not be aware of the\n",
    "complexity, just as most drivers are able to use cars without\n",
    "understanding the details of each part. In the ideal case a PPL the user\n",
    "should just feel as though things work just the way they want them. This\n",
    "is the challenge PPL designers must meet.\n",
    "\n",
    "In the remaining of the chapter, we will give some overview of some\n",
    "general components of a PPL, with examples of design choice from\n",
    "different PPLs. We are not aiming to provide an exhaustive descriptions\n",
    "of all PPLs [^2], and we are also not trying to convince you to develop\n",
    "a PPL [^3]. Rather, by understanding the implementation consideration,\n",
    "we hope that you will gain a better understanding of how to write more\n",
    "performative Bayesian models, and to diagnose computation bottlenecks\n",
    "and errors when they occur.\n",
    "\n",
    "[^1]: With the prerequisite that basic ingredients like APIs to specify\n",
    "    probability distribution and random variables, basic numerical\n",
    "    transformations are already implemented.\n",
    "\n",
    "[^2]: Even Wikipedia only contains a partial list\n",
    "    <https://en.wikipedia.org/wiki/Probabilistic_programming#List_of_probabilistic_programming_languages>.\n",
    "\n",
    "[^3]: *An Introduction to Probabilistic Programming* by van de Meent et\n",
    "    al {cite:p}`van2018introduction` is a good starting point if you are\n",
    "    interested in both PPL development and usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(example-rainier)=\n",
    "\n",
    "### 10.1.1. Example: Rainier\n",
    "\n",
    "Consider the development of Rainier [^4], a PPL written in Scala\n",
    "developed at Stripe. Stripe is a payments processing company that\n",
    "handles finances for many thousands of partner business. In Stripe, they\n",
    "need to estimate the distribution of risk associated with each partnered\n",
    "business, ideally with a PPL that is able to support many parallel\n",
    "inferences (one per each business partner) and easy to deploy in\n",
    "Stripe's compute cluster. As Stripe's compute clusters included a Java\n",
    "run time environment, they choose Scala as it can be compiled to Java\n",
    "bytecode. Rainier. In this case PyMC3 and Stan were considered as well,\n",
    "but due to either the restriction of Python use (PyMC3), or the\n",
    "requirement for a C++ compiler (Stan), creating a PPL for their\n",
    "particular use case was the best choice.\n",
    "\n",
    "Most users will not need to develop their own PPL but we present this\n",
    "case study to highlight how considerations of both the environment in\n",
    "which you are using the code, and the functionality of the available\n",
    "PPLs can help inform a decision for a smoother experience as a\n",
    "\n",
    "[^4]: <https://github.com/stripe/rainier>. A more in-depth reflection of\n",
    "    the development of Rainer is described in a podcast\n",
    "    <https://www.learnbayesstats.com/episode/22-eliciting-priors-and-doing-bayesian-inference-at-scale-with-avi-bryant>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. Posterior Computation\n",
    "\n",
    "Inference is defined as a conclusion reached on the basis of evidence\n",
    "and reasoning and the posterior computational methodology is the engine\n",
    "that gets us to the conclusion. The posterior computation method can\n",
    "largely be thought of as two parts, the computation algorithm, and the\n",
    "software and hardware that makes the calculation, often referred to as\n",
    "the computational backend. When either designing or selecting a PPL, the\n",
    "available posterior computation methods ends up being a key decision\n",
    "that informs many factors of the workflow, from the speed of inference,\n",
    "hardware needed, complexity of PPL, and breadth of applicability. There\n",
    "are numerous algorithms to compute the posterior [^5], from exact\n",
    "computations when using conjugate models, to numerical approximations\n",
    "like grid search to Hamiltonian Monte Carlo (HMC), to model\n",
    "approximation like Laplace approximation and variational inference\n",
    "(covered in more detail in Section {ref}`vi_details`). When selecting\n",
    "an inference algorithms both the PPL designer and the user need to make\n",
    "a series of choices. For the PPL designer the algorithms have different\n",
    "levels of implementation complexity. For example, conjugate methods are\n",
    "quite easy to implement, as there exists analytical formulas that can be\n",
    "written in a couple lines of code, whereas MCMC samplers are more\n",
    "complex, typically requiring a PPL designer to write much more code than\n",
    "an analytical solution. Another tradeoff in computational complexity,\n",
    "conjugate methods do not require much computation power and can return a\n",
    "posterior in sub millisecond on all modern hardware, even a cell phone.\n",
    "By comparison, HMC is slow and require a system that can compute\n",
    "gradients, such as the one we will present in Section {ref}`auto_grad`. This\n",
    "limits HMC computation to relatively powerful computers, sometimes with\n",
    "specialized hardware.\n",
    "\n",
    "The user faces a similar dilemma, more advanced posterior computation\n",
    "methods are more general and require less mathematical expertise, but\n",
    "require more knowledge to assess and ensure correct fit. We have seen\n",
    "this throughout this book, where visual and numerical diagnostics are\n",
    "necessary to ensure our MCMC samplers have converged to an *estimate* of\n",
    "the posterior. Conjugate models do not need any convergence diagnostics\n",
    "due to the fact they calculate the posterior *exactly*, every time if\n",
    "the right mathematics are used.\n",
    "\n",
    "For all these reasons there is no universal recommendation for an\n",
    "inference algorithm that suits every situation. At time of writing MCMC\n",
    "methods, especially adaptive Dynamic Hamiltonian Monte Carlo, are the\n",
    "most flexible, but not useful in all situations. As a user it is\n",
    "worthwhile understanding the availability and tradeoffs of each\n",
    "algorithm to be able to make an assessment for each situation.\n",
    "\n",
    "[^5]: See Section {ref}`inference_methods` for a discussion of\n",
    "    some of more prevalent posterior computation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(auto_grad)=\n",
    "\n",
    "### 10.2.1. Getting the Gradient\n",
    "\n",
    "An incredibly useful piece of information in computational mathematics\n",
    "is the gradient. Also known as the slope, or the derivative for one\n",
    "dimensional functions, it indicates how fast a function output value is\n",
    "changing at any point in its domain. By utilizing the gradient many\n",
    "algorithms are developed to more efficiently achieve their goal. With\n",
    "inference algorithms we have seen this difference when comparing the\n",
    "Metropolis Hasting algorithm, which does not need a gradient when\n",
    "sampling, to Hamiltonian Monte Carlo, which does use the gradient and\n",
    "usually returns high quality samples faster [^6].\n",
    "\n",
    "Just as Markov chain Monte Carlo was originally developed in the sub\n",
    "field of statistical mechanics before computational Bayesians adopted\n",
    "it, most of the gradient evaluation libraries were originally developed\n",
    "as part of \"Deep Learning\\\" libraries mostly intended for\n",
    "backpropagation computation to train Neural Networks. These include\n",
    "Theano, TensorFlow and PyTorch. Bayesians however, learned to use them\n",
    "as computational backends for Bayesian Inference. An example of\n",
    "computational gradient evaluation using JAX {cite:p}`jax2018github`, a\n",
    "dedicated autograd library, shown in Code Block\n",
    "[jax_grad_small](jax_grad_small). In this Code Block the\n",
    "gradient of $x^2$ is computed at a value of $4$. We can solve this\n",
    "analytically with the rule $rx^{r-1}$, and we can then calculate\n",
    "$2*4=8$. However, with autograd libraries users do not need to think\n",
    "about closed form solutions. All that is needed is an expression of the\n",
    "function itself and the computer can automatically calculate the\n",
    "gradient, as implied by \"auto\\\" in autograd.\n",
    "\n",
    "```{code-block} python\n",
    ":name: jax_grad_small\n",
    ":caption: jax_grad_small\n",
    "\n",
    "from jax import grad\n",
    "\n",
    "simple_grad = grad(lambda x: x**2)\n",
    "print(simple_grad(4.0))\n",
    "```\n",
    "\n",
    "```none\n",
    "8.0\n",
    "```\n",
    "\n",
    "[^6]: In terms of effective samples per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Run 2025-01-28 00:37:11.425317\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "\n",
    "import datetime\n",
    "print(f\"Last Run {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.style.use(\"arviz-grayscale\")\n",
    "plt.rcParams[\"figure.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 00:37:11.847885: W external/xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.5 which is older than the PTX compiler version 12.6.85. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "from jax import grad\n",
    "\n",
    "simple_grad = grad(lambda x: x**2)\n",
    "print(simple_grad(4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods such us Adaptive Dynamic Hamiltonian Monte Carlo or Variational\n",
    "Inference use gradients to estimate posterior distributions. Being able\n",
    "to obtain gradient easily becomes even more important when we realize\n",
    "that in posterior computation the gradient typically gets computed\n",
    "thousands of times. We show one such calculation in Code Block\n",
    "[jax_model_grad](jax_model_grad) using JAX for a small\n",
    "\"hand built\\\" model.\n",
    "\n",
    "``` {code-block} python\n",
    ":name: jax_model_grad\n",
    ":caption: jax_model_grad\n",
    "\n",
    "from jax import grad\n",
    "from jax.scipy.stats import norm\n",
    "\n",
    "def model(test_point, observed):\n",
    "    z_pdf = norm.logpdf(test_point, loc=0, scale=5)\n",
    "    x_pdf = norm.logpdf(observed, loc=test_point, scale=1)\n",
    "    logpdf = z_pdf + x_pdf\n",
    "    return logpdf\n",
    "\n",
    "model_grad = grad(model)\n",
    "\n",
    "observed, test_point = 5.0, 2.5 \n",
    "logp_val = model(test_point, observed)\n",
    "grad = model_grad(test_point, observed)\n",
    "print(f\"log_p_val: {logp_val}\")\n",
    "print(f\"grad: {grad}\")\n",
    "```\n",
    "\n",
    "```none\n",
    "log_p_val: -6.697315216064453\n",
    "grad: 2.4000000953674316\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_p_val: -6.697315216064453\n",
      "grad: 2.4000000953674316\n"
     ]
    }
   ],
   "source": [
    "from jax import grad\n",
    "from jax.scipy.stats import norm\n",
    "\n",
    "def model(test_point, observed):\n",
    "    z_pdf = norm.logpdf(test_point, loc=0, scale=5)\n",
    "    x_pdf = norm.logpdf(observed, loc=test_point, scale=1)\n",
    "    logpdf = z_pdf + x_pdf\n",
    "    return logpdf\n",
    "\n",
    "model_grad = grad(model)\n",
    "\n",
    "observed, test_point = 5.0, 2.5 \n",
    "logp_val = model(test_point, observed)\n",
    "grad = model_grad(test_point, observed)\n",
    "print(f\"log_p_val: {logp_val}\")\n",
    "print(f\"grad: {grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison we can make the same calculation using a PyMC3 model and\n",
    "computing the gradient using Theano in Code Block\n",
    "[pymc3_model_grad](pymc3_model_grad).\n",
    "\n",
    "``` {code-block} python\n",
    ":name: pymc3_model_grad\n",
    ":caption: pymc3_model_grad\n",
    "\n",
    "with pm.Model() as model:\n",
    "    z = pm.Normal(\"z\", 0., 5.)\n",
    "    x = pm.Normal(\"x\", mu=z, sd=1., observed=observed)\n",
    "\n",
    "func = model.logp_dlogp_function()\n",
    "func.set_extra_values({})\n",
    "print(func(np.array([test_point])))\n",
    "```\n",
    "\n",
    "```none\n",
    "[array(-6.69731498), array([2.4])]\n",
    "```\n",
    "\n",
    "\n",
    "From the output we can see the PyMC3 model returns the same logp and\n",
    "gradient as the JAX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(-6.69731501), array([2.4])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwting/anaconda3/envs/rapids/lib/python3.11/site-packages/pymc/model/core.py:250: UserWarning: ValueGradFunction will become a function of raveled inputs.\n",
      "Specify `ravel_inputs` to suppress this warning. Note that setting `ravel_inputs=False` will be forbidden in a future release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as model:\n",
    "    z = pm.Normal(\"z\", 0., 5.)\n",
    "    x = pm.Normal(\"x\", mu=z, sigma=1., observed=observed)\n",
    "\n",
    "func = model.logp_dlogp_function()\n",
    "func.set_extra_values({})\n",
    "print(func(np.array([test_point])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.2. Example: Near Real Time Inference\n",
    "\n",
    "As a hypothetical example consider a statistician at a credit card\n",
    "company that is concerned with detecting credit card fraud quickly so it\n",
    "can disable cards before the thief can make more transactions. A\n",
    "secondary system classifies transactions as fraudulent or legitimate but\n",
    "the company wants to ensure it does not block cards with a low number of\n",
    "events and wants to be able to set priors for different customers to\n",
    "control the sensitivity. It is decided that the users accounts will be\n",
    "disabled when mean of the posterior distribution is above a probability\n",
    "threshold of 50%. In this near real time scenario inference needs to be\n",
    "performed in less than a second so fraudulent activity can be detected\n",
    "before the transaction clears. The statistician recognizes that this can\n",
    "be analytically expressed using a conjugate model which she then writes\n",
    "in Equation {eq}`eq:conjugate_beta_fraud`, where the $\\alpha$ and\n",
    "$\\beta$ parameters, representing the prior of fraud and non-fraud\n",
    "transactions directly. As transactions are observed they are used fairly\n",
    "directly compute the posterior parameters.\n",
    "\n",
    "```{math} \n",
    ":label: eq:conjugate_beta_fraud\n",
    "\\begin{split}\n",
    "    \\alpha_\\text{post} &= \\alpha_\\text{prior} + \\texttt{fraud\\_observations} \\\\\n",
    "    \\beta_\\text{post} &=  \\beta_\\text{prior} + \\texttt{non\\_fraud\\_observations} \\\\\n",
    "   % p(\\theta)  &= \\text{Beta}(fraud\\_prior, non\\_fraud\\_prior) \\\\\n",
    "    p(\\theta \\mid y)  &= \\text{Beta}(\\alpha_\\text{post}, \\beta_\\text{post}) \\\\\n",
    "    \\mathop{\\mathbb{E}}[p(\\theta \\mid y)] &= \\frac{\\alpha_\\text{post}}{\\alpha_\\text{post} + \\beta_\\text{post}}\n",
    "\n",
    "\\end{split}\n",
    "```\n",
    "\n",
    "She can then fairly trivially express these calculations in Python as\n",
    "shown in Code Block [fraud_detector](fraud_detector). No\n",
    "external libraries needed either, making this function quite easy to\n",
    "deploy.\n",
    "\n",
    "``` {code-block} ipython3\n",
    ":name: fraud_detector\n",
    ":caption: fraud_detector\n",
    "\n",
    "def fraud_detector(fraud_observations, non_fraud_observations, fraud_prior=8, non_fraud_prior=6):\n",
    "    \"\"\"Conjugate Beta Binomial model for fraud detection\"\"\"\n",
    "    expectation = (fraud_prior + fraud_observations) / (\n",
    "        fraud_prior + fraud_observations + non_fraud_prior + non_fraud_observations)\n",
    "    \n",
    "    if expectation > .5:\n",
    "        return {\"suspend_card\":True}\n",
    "\n",
    "%timeit fraud_detector(2, 0)\n",
    "```\n",
    "\n",
    "```none\n",
    "152 ns ± 0.969 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 ns ± 2.25 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def fraud_detector(fraud_observations, non_fraud_observations, fraud_prior=8, non_fraud_prior=6):\n",
    "    \"\"\"Conjugate Beta Binomial model for fraud detection\"\"\"\n",
    "    expectation = (fraud_prior + fraud_observations) / (\n",
    "        fraud_prior + fraud_observations + non_fraud_prior + non_fraud_observations)\n",
    "    \n",
    "    if expectation > .5:\n",
    "        return {\"suspend_card\":True}\n",
    "\n",
    "%timeit fraud_detector(2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To meet the sensitivity and probability computation time requirements of\n",
    "less than one second a conjugate prior is selected, and the model\n",
    "posterior is calculated in Code Block\n",
    "[fraud_detector](fraud_detector). The calculations took\n",
    "about 152 ns, in contrast an MCMC sampler will take around 2 seconds on\n",
    "the same machine which is over 6 orders of magnitude. It is unlikely\n",
    "that any MCMC sampler would meet the time requirement needed from this\n",
    "system, making a conjugate prior the clear choice.\n",
    "\n",
    "::: {admonition} Hardware and Sampling Speed\n",
    "\n",
    "From a hardware perspective, there are three\n",
    "ways to increase MCMC sampling speed. The first is typically the clock\n",
    "speed of the processing unit, often measured in hertz, or Gigahertz for\n",
    "modern computers. This is the speed at which the instructions are\n",
    "executed, so generally speaking a 4 Gigahertz computer can execute twice\n",
    "as many instructions in a second than a 2 Gigahertz computer. In MCMC\n",
    "sampling the clock speed will correlate with the number of samples that\n",
    "can be taken in a timespan for a single chain. The other is\n",
    "parallelization across multiple cores in a processing unit. In MCMC\n",
    "sampling each chains can be sampled in parallel on computers with\n",
    "multiple cores. This coincidentally is convenient as many convergence\n",
    "metrics use multiple chains. On a modern desktop computer anywhere from\n",
    "2 to 16 cores are typically available. The last method is specialized\n",
    "hardware such as the Graphics Processing Units (GPUs) and Tensor\n",
    "Processing Units (TPUs). If paired with the correct software and\n",
    "algorithms these are able to both sample each chain more quickly, but\n",
    "also sample more chains in parallel.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(application-programming-interfaces)=\n",
    "\n",
    "## 10.3. Application Programming Interfaces\n",
    "\n",
    "Application Programming Interfaces (API) \"define interactions between\n",
    "multiple software intermediaries\\\". In the Bayesian case the most narrow\n",
    "definition is the interactions between the user and the method to\n",
    "compute the posterior. At its most broad it can include multiple steps\n",
    "in the Bayesian workflow, such as specifying a random variable with a\n",
    "distribution, linking different random variables to create the model,\n",
    "prior and posterior predictive checks, plotting, or any task. The API is\n",
    "typically first part, and sometimes the only part, a PPL practitioner\n",
    "interactions with and typically is where the practitioner spends the\n",
    "most amount of time. API design is both a science and an art and\n",
    "designers must balance multiple concerns.\n",
    "\n",
    "On the science side PPLs must be able to interface with a computer and\n",
    "provide the necessary elements to control the computation methods. Many\n",
    "PPLs are defined with base languages and typically need to follow the\n",
    "fixed computational constraints of the base language as well as the\n",
    "computational backend. In\n",
    "Section {ref}`conjugate_case_study`, only a 4 parameters and one line of code\n",
    "were needed to obtain an exact result. Contrast this with MCMC examples\n",
    "that had various inputs such as number of draws, acceptance rate, number\n",
    "of tuning steps and more. The complexity of MCMC, while mostly hidden,\n",
    "still surfaced additional complexity in the API.\n",
    "\n",
    "::: {admonition} So many APIs, so many interfaces\n",
    "\n",
    "In a modern Bayesian workflow there is\n",
    "not just the PPL API but APIs of all the supporting packages in the\n",
    "workflow. In this book we have also used the Numpy, Matplotlib, Scipy,\n",
    "Pandas and ArviZ APIs across examples, not to mention the Python API\n",
    "itself. In the Python ecosystem. These choices of packages, and the APIs\n",
    "they bring, are also subject to personal choice. A practitioner may\n",
    "choose to use Bokeh as a replacement to Matplotlib for plotting, or\n",
    "xarray in addition to pandas, and in doing so the user will need to\n",
    "learn those APIs as well.\n",
    "\n",
    "In addition to just APIs there are many code writing interfaces to write\n",
    "Bayesian models, or just code in general. Code can be written in the\n",
    "text editors, notebook, Integrated Development Environments (IDEs), or\n",
    "the command line directly.\n",
    "\n",
    "The use of these tools, both the supporting packages and the coding\n",
    "interface, is not mutually exclusive. For someone new to computational\n",
    "statistics this can be a lot to take in. When starting out we suggest\n",
    "using a simple text editor and a few supporting packages to allow for\n",
    "your focus to be on the code and model, before moving onto more complex\n",
    "interfaces such as notebooks or Integrated Development Environments. We\n",
    "provide more guidance regarding this topic in Section {ref}`dev_environment`.\n",
    ":::\n",
    "\n",
    "On the art side API is the interface for human users. This interface is\n",
    "one of the most important parts of the PPL. Some users tend to have\n",
    "strong, albeit subjective views, about design choices. Users want the\n",
    "simplest, most flexible, readable, and easy to write API, objectives\n",
    "which, for the poor PPL designer, are both ill defined and opposed with\n",
    "each other. One choice a PPL designer can make is to mirror style and\n",
    "functionality of the base language. For example, there is a notion of\n",
    "\"Pythonic\\\" programs which are follow in a certain style [^7]. This\n",
    "notion of pythonic API is what informs the PyMC3 API, the goal is to\n",
    "explicitly have users feel like they are writing their models in Python.\n",
    "In contrast Stan models are written in a domain specific language\n",
    "informed by other PPLs such as BUGS {cite:p}`gilks_thomas_spiegelhalter_1994`\n",
    "and languages such as C++ {cite:p}`carpenter_2017`. The Stan language includes\n",
    "notable API primitives such as curly braces and uses a block syntax as\n",
    "shown in Code Block [code_stan](code_stan). Writing a Stan model\n",
    "distinctly *does not* feel like writing Python, but this is not a knock\n",
    "against the API. It is just a different choice from a design standpoint\n",
    "and a different experience for the user.\n",
    "\n",
    "[^7]: The Zen of Python details the philosophy behind the idea of\n",
    "    pythonic design <https://www.python.org/dev/peps/pep-0020/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(example-stan-and-slicstan)=\n",
    "\n",
    "### 10.3.1. Example: Stan and Slicstan\n",
    "\n",
    "Depending on the use case, user might prefer different level of\n",
    "abstraction in terms model specification, independent of any other PPL\n",
    "component. Stan and Slicstan using from Gorinova etal {cite:p}`Gorinova_2019]\n",
    "which is specifically dedicated to studying and proposing Stan APIs. In\n",
    "Code Block [code_stan](code_stan) we show the, original, Stan model\n",
    "syntax. In the Stan syntax various pieces of a Bayesian model are\n",
    "indicated by blocks declarations. These names correspond with the\n",
    "various sections of the workflow, such as specifying the model and\n",
    "parameters, data transformations, prior and posterior predictive\n",
    "sampling, with corresponding names such as parameters, model,\n",
    "transformed parameters, and generated quantities.\n",
    "\n",
    "```{code-block} none\n",
    ":name: code_stan\n",
    ":caption: code_stan\n",
    "\n",
    "parameters {\n",
    "    real y_std;\n",
    "    real x_std;\n",
    "}\n",
    "transformed parameters {\n",
    "    real y = 3 * y_std;\n",
    "    real x = exp(y/2) * x_std;\n",
    "}\n",
    "model {\n",
    "    y_std ~ normal(0, 1);\n",
    "    x_std ~ normal(0, 1);\n",
    "}\n",
    "```\n",
    "\n",
    "An alternative syntax for Stan models is Slicstan {cite:p}`Gorinova_2019`, the\n",
    "same model of which is shown in Code Block\n",
    "[slicstan](slicstan). Slicstan provides a compositional\n",
    "interface to Stan, letting users define functions which can be named and\n",
    "reused, and does away with the block syntax. These features mean\n",
    "Slicstan programs can be expressed in less code than standard Stan\n",
    "models. While not always the most important metric less code means less\n",
    "code that the Bayesian modeler needs to write, and less code that a\n",
    "model reviewer needs to read. Also, like Python, composable functions\n",
    "allow the user to define an idea once and reuse it many times, such as\n",
    "`my_normal` in the Slicstan snippet.\n",
    "\n",
    "```{code-block} none\n",
    ":name: slicstan\n",
    ":caption: slicstan\n",
    "\n",
    "real my_normal(real m, real s) {\n",
    "real std ~ normal(0, 1);\n",
    "    return s * std + m;\n",
    "}\n",
    "real y = my_normal(0, 3);\n",
    "real x = my_normal(0, exp(y/2));\n",
    "```\n",
    "\n",
    "For the original Stan syntax, it has the benefit of familiarity (for\n",
    "those who already use it) and documentation. The familiarity may come\n",
    "from Stan's choice to model itself after BUGS ensuring that users who\n",
    "have prior experience with that language, will be comfortable\n",
    "transitioning to the Stan syntax. It also is familiar for people who\n",
    "have been using Stan for numerous years. Since it was released 2012,\n",
    "there have now been multiple years for users to get familiar with the\n",
    "language, publish examples, and write models. For new users the block\n",
    "model forces organization so when writing a Stan program they will end\n",
    "up being more consistent.\n",
    "\n",
    "Note both Stan and Slicstan use the same codebase under the API layer\n",
    "the difference in API is solely for the benefit of the user. In this\n",
    "case which API is \"better\\\" is a choice for each user. We should note\n",
    "this case study is only a shallow discussion of the Stan API. For full\n",
    "details we suggest reading the full paper, which formalizes both sets of\n",
    "syntax and shows the level of detail goes into API design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(example-pymc3-and-pymc4)=\n",
    "\n",
    "### 10.3.2. Example: PyMC3 and PyMC4\n",
    "\n",
    "Our second API is a case study of an API design change that was required\n",
    "because of a computational backend change, in this case from Theano in\n",
    "PyMC3 to TensorFlow in PyMC4 a PPL that was initially intended to\n",
    "replace PyMC3 {cite:p}`kochurovpymc4`. In the design of PyMC4 the designers of\n",
    "the language desired to keep the syntax *as close as possible* to the\n",
    "PyMC3 syntax. While the inference algorithms remained the same, the\n",
    "fundamental way in which TensorFlow and Python works meant the PyMC4 API\n",
    "forced into a particular design due to the change in computational\n",
    "backend. Consider the Eight Schools model {cite:p}`rubin_1981` implemented in\n",
    "PyMC3 syntax in Code Block [pymc3_schools](pymc3_schools)\n",
    "and the now, defunct [^8] PyMC4 syntax in Code Block\n",
    "[pymc4_schools](pymc4_schools).\n",
    "\n",
    "```{code-block} python\n",
    ":name: pymc3_schools\n",
    ":caption: pymc3_schools\n",
    "\n",
    "with pm.Model() as eight_schools_pymc3:\n",
    "    mu = pm.Normal(\"mu\", 0, 5)\n",
    "    tau = pm.HalfCauchy(\"tau\", 5)\n",
    "    theta = pm.Normal(\"theta\", mu=mu, sigma=tau, shape=8)\n",
    "    obs = pm.Normal(\"obs\", mu=theta, sigma=sigma, observed=y)\n",
    "```\n",
    "\n",
    "``` {code-block} python\n",
    ":name: pymc4_schools\n",
    ":caption: pymc4_schools\n",
    "@pm.model\n",
    "def eight_schools_pymc4():\n",
    "    mu = yield pm.Normal(\"mu\", 1, 5)\n",
    "    tau = yield pm.HalfNormal(\"tau\", 5)\n",
    "    theta = yield pm.Normal(\"theta\", loc=mu, scale=sigma, batch_stack=8)\n",
    "\n",
    "    obs = yield pm4.Normal(\"obs\", loc=theta, scale=sigma, observed=y)\n",
    "    return obs\n",
    "```\n",
    "\n",
    "The differences in PyMC4 is the decorator `@pm.model`, the declaration\n",
    "of a Python function, the use of generators indicated by `yield`, and\n",
    "differing argument names. You may have noticed that the `yield` is the\n",
    "same that you have seen in the TensorFlow Probability code. In both PPLs\n",
    "`yield` statement was a necessary part of the API due to the choice\n",
    "coroutine. These APIs changes were not desired however, as users would\n",
    "have to learn a new syntax, all existing PyMC3 code would have to be\n",
    "rewritten to use PyMC4, and all existing PyMC3 documentation would\n",
    "become obsolete. This is an example where the API is informed not by\n",
    "user preference, but by the choice computational backend used to\n",
    "calculate the posterior. In the end the feedback from users to keep the\n",
    "PyMC3 API unchanged was one of the reasons to terminate PyMC4\n",
    "development.\n",
    "\n",
    "[^8]: <https://pymc-devs.medium.com/the-future-of-pymc3-or-theano-is-dead-long-live-theano-d8005f8a0e9b.>\n",
    "    contains more details about the decision and future road map of\n",
    "    PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ppl-driven-transformations)=\n",
    "\n",
    "## 10.4. PPL Driven Transformations\n",
    "\n",
    "In this book we saw many mathematical transformations that allowed us to\n",
    "define a variety of models, easily and with great flexibility such as\n",
    "GLMs. Or we saw transformations that allowed us to make results more\n",
    "interpretable such as centering. In this section we will specifically\n",
    "discuss transformations that are driven more specifically by PPL. They\n",
    "are sometimes a bit implicit and we will discuss two examples in this\n",
    "section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(log_probabilities)=\n",
    "\n",
    "### 10.4.1. Log Probabilities\n",
    "\n",
    "One of the most common transformations is the log probability transform.\n",
    "To understand why let us go through an example where we calculate an\n",
    "arbitrary likelihood. Assume we observe two independent outcomes $y_0$\n",
    "and $y_1$, their joint probability is:\n",
    "\n",
    "```{math} \n",
    ":label: eq:expanded_likelihood\n",
    "p(y_0, y_1 \\mid \\boldsymbol{\\theta}) = p(y_0 \\mid \\boldsymbol{\\theta})p(y_1 \\mid \\boldsymbol{\\theta})\n",
    "    \n",
    "```\n",
    "\n",
    "To give a specific situation let us say we observed the value 2 twice\n",
    "and we decide to use a Normal distribution as a likelihood in our model.\n",
    "We can specify our model by expanding Equation\n",
    "{eq}`eq:expanded_likelihood` into:.\n",
    "\n",
    "```{math} \n",
    ":label: eq:expanded_likelihood_normal\n",
    "\\mathcal{N}(2, 2 \\mid \\mu=0,\\sigma=1) = \\mathcal{N}(2 \\mid 0,1)\\mathcal{N}(2 \\mid 0,1)\n",
    "    \n",
    "```\n",
    "\n",
    "Being computational statisticians we can now calculate this value with a\n",
    "little bit of code.\n",
    "\n",
    "```{code-block} python\n",
    ":name: two_observed\n",
    ":caption: two_observed\n",
    "\n",
    "observed = np.repeat(2, 2)\n",
    "pdf = stats.norm(0, 1).pdf(observed)\n",
    "np.prod(pdf, axis=0)\n",
    "```\n",
    "\n",
    "```none\n",
    "0.0029150244650281948\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0029150244650281948"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed = np.repeat(2, 2)\n",
    "pdf = stats.norm(0, 1).pdf(observed)\n",
    "np.prod(pdf, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With two observations we get 20 decimals of precision for joint\n",
    "probability density in Code Block\n",
    "[two_observed](two_observed) with no issues, but now let\n",
    "us assume we see a total of 1000 observations, all of the same value of\n",
    "2. We can repeat our calculation in Code Block\n",
    "[thousand_observed](thousand_observed). This time however,\n",
    "we have an issue, Python reports a joint probability density of 0.0,\n",
    "which cannot be true.\n",
    "\n",
    "```{code-block} python\n",
    ":name: thousand_observed\n",
    ":caption: thousand_observed\n",
    "\n",
    "observed = np.repeat(2, 1000)\n",
    "pdf = stats.norm(0, 1).pdf(observed)\n",
    "np.prod(pdf, axis=0)\n",
    "```\n",
    "\n",
    "```none\n",
    "0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed = np.repeat(2, 1000)\n",
    "pdf = stats.norm(0, 1).pdf(observed)\n",
    "np.prod(pdf, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are seeing is an example of *floating point precision* error in\n",
    "computers. Due to the fundamental way computers store numbers in memory\n",
    "and evaluate calculations, only a limited amount of precision is\n",
    "possible. In Python this error in precision is often hidden from the\n",
    "user [^9], although in certain cases the user is exposed to the lack of\n",
    "precision, as shown in Code Block\n",
    "[imperfect_subtract](imperfect_subtract)\n",
    "\n",
    "```{code-block} python\n",
    ":name: imperfect_subtract\n",
    ":caption: imperfect_subtract\n",
    "\n",
    "1.2 - 1\n",
    "```\n",
    "```none\n",
    "0.19999999999999996\n",
    "```\n",
    "\n",
    "[^9]: <https://docs.Python.org/3/tutorial/floatingpoint.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19999999999999996"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05399096651318806, 0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf[0], np.prod(pdf, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With relatively \"large\\\" numbers the small error in the far decimal\n",
    "place matter little. However, in Bayesian modeling we often are working\n",
    "with very small float point numbers, and even worse we multiply them\n",
    "together many times making them smaller yet. To mitigate this problem\n",
    "PPLs perform a log transformation of probability often abbreviated to\n",
    "*logp*. Thus expression {eq}`eq:expanded_likelihood` turns into:\n",
    "\n",
    "```{math} \n",
    ":label: eq:expanded_loglikelihood\n",
    "\\log (p(y_0, y_1 \\mid \\boldsymbol{\\theta})) = \\log (p(y_0 \\mid \\boldsymbol{\\theta}))+ \\log (p(y_1 \\mid \\boldsymbol{\\theta}))\n",
    "    \n",
    "```\n",
    "\n",
    "This has two effects, it makes small numbers relatively large, and due\n",
    "to the product rule of logarithms, changes the multiplication into a\n",
    "sum. Using the same example but performing the calculation in log space,\n",
    "we see a more numerically stable result in Code Block\n",
    "[log_transform](log_transform).\n",
    "\n",
    "```{code-block} python\n",
    ":name: log_transform\n",
    ":caption: log_transform\n",
    "\n",
    "logpdf = stats.norm(0, 1).logpdf(observed)\n",
    "\n",
    "# Compute individual logpdf two ways for one observation, as well as total\n",
    "np.log(pdf[0]), logpdf[0], logpdf.sum()\n",
    "```\n",
    "\n",
    "```none\n",
    "(-2.9189385332046727, -2.9189385332046727, -2918.9385332046736)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.9189385332046727, -2.9189385332046727, -2918.9385332046736)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logpdf = stats.norm(0, 1).logpdf(observed)\n",
    "np.log(pdf[0]), logpdf[0], logpdf.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.9189385332046727"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(pdf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(random-variables-and-distributions-transformations)=\n",
    "\n",
    "### 10.4.2. Random Variables and Distributions Transformations\n",
    "\n",
    "Random variables that distributed as a bounded distributions, like the\n",
    "Uniform distribution that is specified with a fixed interval $[a, b]$,\n",
    "present a challenge for gradient evaluation and samplers based on them.\n",
    "Sudden changes in geometry make it difficult to sample the distribution\n",
    "at the neighborhood of those sudden changes. Imagine rolling a ball down\n",
    "a set of stairs or a cliff, rather than a smooth surface. It is easier\n",
    "to estimate the trajectory of the ball over a smooth surface rather than\n",
    "the discontinuous surface. Thus, another useful set of transformations\n",
    "in PPLs [^10] are transformations that turn bounded random variables,\n",
    "such as those distributed as Uniform, Beta, Halfnormal, etc, into\n",
    "unbounded random variables that span entire real line from ($-\\infty$,\n",
    "$\\infty$). These transformations however, need to be done with care as\n",
    "we must now correct for the volume changes in our transformed\n",
    "distributions. To do so we need to compute the Jacobians of the\n",
    "transformation and accumulate the calculated log probabilities,\n",
    "explained in further detail in Section {ref}`transformations`.\n",
    "\n",
    "PPLs usually transform bounded random variables into unbounded random\n",
    "variables and perform inference in the unbounded space, and then\n",
    "transform back the values to the original bounded space, all can happen\n",
    "without user input. Thus users do not need to interact with these\n",
    "transformations if they do not want to. For a concrete example both the\n",
    "forward and backward transform for the Uniform random variable are shown\n",
    "in Equation {eq}`eq:interval_transform` and computed in Code Block\n",
    "[interval_transform](interval_transform). In this\n",
    "transformation the lower and upper bound $a$ and $b$ are mapped to\n",
    "$-\\infty$ and $\\infty$ respectively, and the values in between are\n",
    "\"stretched\\\" to values in between accordingly.\n",
    "\n",
    "```{math} \n",
    ":label: eq:interval_transform\n",
    "\\begin{split}\n",
    "    x_t =& \\log (x - a) - \\log(b-x)\\\\\n",
    "    x =& a + \\frac{1}{1 + e^{-x_t}} (b-a)\n",
    "    \n",
    "\\end{split}\n",
    "```\n",
    "\n",
    "```{code-block} python\n",
    ":name: interval_transform\n",
    ":caption: interval_transform\n",
    "\n",
    "lower, upper = -1, 2\n",
    "domain = np.linspace(lower, upper, 5)\n",
    "transform = np.log(domain - lower) - np.log(upper - domain)\n",
    "print(f\"Original domain: {domain}\")\n",
    "print(f\"Transformed domain: {transform}\")\n",
    "```\n",
    "\n",
    "```none\n",
    "Original domain:[-1.   -0.25  0.5   1.25  2.  ]\n",
    "Transformed domain: [-inf -1.09861229, 0., 1.09861229, inf]\n",
    "```\n",
    "\n",
    "[^10]: <https://mc-stan.org/docs/2_25/reference-manual/variable-transforms-chapter.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original domain: [-1.   -0.25  0.5   1.25  2.  ]\n",
      "Transformed domain: [       -inf -1.09861229  0.          1.09861229         inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_320854/2568761612.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  transform = np.log(domain - lower) - np.log(upper - domain)\n"
     ]
    }
   ],
   "source": [
    "lower, upper = -1, 2\n",
    "domain = np.linspace(lower, upper, 5)\n",
    "transform = np.log(domain - lower) - np.log(upper - domain)\n",
    "print(f\"Original domain: {domain}\")\n",
    "print(f\"Transformed domain: {transform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automatic transform can be seen by adding a Uniform random variable\n",
    "to a PyMC3 model and inspecting the variable and the underlying\n",
    "distribution in the model object, as shown in Code Block\n",
    "[uniform_transform](uniform_transform).\n",
    "\n",
    "```{code-block} python\n",
    ":name: uniform_transform\n",
    ":caption: uniform_transform\n",
    "\n",
    "with pm.Model() as model:\n",
    "    x = pm.Uniform(\"x\", -1., 2.)\n",
    "\n",
    "model.vars\n",
    "```\n",
    "\n",
    "```none\n",
    "([x_interval__ ~ TransformedDistribution])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{x_interval__: x ~ Uniform(-1, 2)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Uniform(\"x\", -1., 2.)\n",
    "    \n",
    "model.values_to_rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Variable.eval of __logp_nojac>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.varlogp_nojac.eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing this transform we can also then query the model to check the\n",
    "transformed logp values in Code Block\n",
    "[uniform_transform_logp](uniform_transform_logp). Note how\n",
    "with the transformed distribution we can sample outside of the interval\n",
    "$(-1, 2)$ (the boundaries of the un-transformed Uniform) and still\n",
    "obtain a finite logp value. Also note the logp returned from the\n",
    "`logp_nojac` method, and how the value is the same for the values of -2\n",
    "and 1, and how when we call `logp` the Jacobian adjustment is made\n",
    "automatically.\n",
    "\n",
    "```{code-block} python\n",
    ":name: uniform_transform_logp\n",
    ":caption: uniform_transform_logp\n",
    "\n",
    "print(model.logp({\"x_interval__\":-2}),\n",
    "      model.logp_nojac({\"x_interval__\":-2}))\n",
    "print(model.logp({\"x_interval__\":1}),\n",
    "      model.logp_nojac({\"x_interval__\":1}))\n",
    "```\n",
    "\n",
    "```none\n",
    "-2.2538560220859454 -1.0986122886681098\n",
    "-1.6265233750364456 -1.0986122886681098\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.2538560220859454 -1.0986123\n",
      "-1.6265233750364456 -1.0986123\n"
     ]
    }
   ],
   "source": [
    "print(model.varlogp.eval({\"x_interval__\":-2}),\n",
    "      model.varlogp_nojac.eval({\"x_interval__\":-2}))\n",
    "print(model.varlogp.eval({\"x_interval__\":1}),\n",
    "      model.varlogp_nojac.eval({\"x_interval__\":1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transformation of the probabilities and unbounding of random\n",
    "variables are transformations that PPLs usually apply without most users\n",
    "knowing, but they both have a practical effect on the performance and\n",
    "usability of the PPL across a wide variety of models.\n",
    "\n",
    "There are other more explicit transformations users can perform directly\n",
    "on the distribution itself to construct new distribution. User can then\n",
    "create random variables distributed as these new distributions in a\n",
    "model. For example, the bijectors module {cite:p}`dillon2017tensorflow` in TFP\n",
    "can be used to transform a base distribution into more complicated\n",
    "distributions. Code Block\n",
    "[bijector_lognormal](bijector_lognormal) demonstrates how\n",
    "to construct a $LogNormal(0, 1)$ distribution by transforming the base\n",
    "distribution $\\mathcal{N}(0, 1)$ [^11]. This expressive API design even\n",
    "allows users to define complicated transformation using trainable\n",
    "bijectors (e.g., a neural network {cite:p}`papamakarios2019normalizing`) like\n",
    "`tfb.MaskedAutoregressiveFlow`.\n",
    "\n",
    "```{code-block} python\n",
    ":name: bijector_lognormal\n",
    ":caption: bijector_lognormal\n",
    "\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "lognormal0 = tfd.LogNormal(0., 1.)\n",
    "lognormal1 = tfd.TransformedDistribution(tfd.Normal(0., 1.), tfb.Exp())\n",
    "x = lognormal0.sample(100)\n",
    "\n",
    "np.testing.assert_array_equal(lognormal0.log_prob(x), lognormal1.log_prob(x))\n",
    "```\n",
    "\n",
    "Whether explicitly used, or implicitly applied, we note that these\n",
    "transformations of random variables and distributions are not a strictly\n",
    "required components of PPLs, but certainly are included in nearly every\n",
    "modern PPL in some fashion. For example, they can be incredibly helpful\n",
    "in getting good inference results efficiently, as shown in the next\n",
    "example.\n",
    "\n",
    "[^11]: In fact, this is how `tfd.LogNormal` is implemented in TFP, with\n",
    "    some additional overwrite of class method to make computation more\n",
    "    stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 00:37:21.551682: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-28 00:37:21.551711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-28 00:37:21.552540: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "tfb = tfp.bijectors\n",
    "\n",
    "lognormal0 = tfd.LogNormal(0., 1.)\n",
    "lognormal1 = tfd.TransformedDistribution(tfd.Normal(0., 1.), tfb.Exp())\n",
    "x = lognormal0.sample(100)\n",
    "\n",
    "np.testing.assert_array_equal(lognormal0.log_prob(x), lognormal1.log_prob(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(example-sampling-comparison-between-bounded-and-unbounded-random-variables)=\n",
    "\n",
    "### 10.4.3. Example: Sampling Comparison between Bounded and Unbounded Random Variables\n",
    "\n",
    "Here we create a small example to demonstrate the differences between\n",
    "sampling from transformed and un-transformed random variable. Data is\n",
    "simulated from a Normal distribution with a very small standard\n",
    "deviation and model is specified in Code Block\n",
    "[case_study_transform](case_study_transform).\n",
    "\n",
    "\n",
    "```{code-block} python\n",
    ":name: case_study_transform\n",
    ":caption: case_study_transform\n",
    "\n",
    "y_observed = stats.norm(0, .01).rvs(20)\n",
    "\n",
    "with pm.Model() as model_transform:\n",
    "    sd = pm.HalfNormal(\"sd\", 5)\n",
    "    y = pm.Normal(\"y\", mu=0, sigma=sd, observed=y_observed)\n",
    "    trace_transform = pm.sample(chains=1, draws=100000)\n",
    "\n",
    "print(model_transform.vars)\n",
    "print(f\"Diverging: {trace_transform.get_sampler_stats('diverging').sum()}\")\n",
    "```\n",
    "\n",
    "```none\n",
    "[sd_log__ ~ TransformedDistribution()]\n",
    "Diverging: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9f1fac5c1949bb82a64ab24ba32c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 1_000 tune and 100_000 draw iterations (1_000 + 100_000 draws total) took 13 seconds.\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{sd_log__: sd, TensorConstant(TensorType(float64, shape=(20,)), data=array([-0. ... 01098095])): y}\n",
      "Diverging: 0\n"
     ]
    }
   ],
   "source": [
    "y_observed = stats.norm(0, .01).rvs(20)\n",
    "\n",
    "with pm.Model() as model_transform:\n",
    "    sd = pm.HalfNormal(\"sd\", 5)\n",
    "    y = pm.Normal(\"y\", mu=0, sigma=sd, observed=y_observed)\n",
    "    idata_transform = pm.sample(chains=1, draws=100000)\n",
    "\n",
    "print(model_transform.values_to_rvs)\n",
    "print(f\"Diverging: {idata_transform.sample_stats['diverging'].sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect free variables and after sampling we can count the number\n",
    "of divergences. From the code output from Code Block\n",
    "[case_study_transform](case_study_transform) we can verify\n",
    "the bounded HalfNormal `sd` variable has been transformed, and in\n",
    "subsequent sampling there are no divergences.\n",
    "\n",
    "For a counterexample, let us specify the same model in Code Block\n",
    "[case_study_no_transform](case_study_no_transform) but in\n",
    "this case the HalfNormal prior distribution explicitly was not\n",
    "transformed. This is reflected both in the model API, as well as when\n",
    "inspecting the models free variables. Subsequent sampling sampling\n",
    "reports 423 divergences.\n",
    "\n",
    "```{code-block} python\n",
    ":name: case_study_no_transform\n",
    ":caption: case_study_no_transform\n",
    "\n",
    "with pm.Model() as model_no_transform:\n",
    "    sd = pm.HalfNormal(\"sd\", 5, transform=None)\n",
    "    y = pm.Normal(\"y\", mu=0, sigma=sd, observed=y_observed)\n",
    "    trace_no_transform = pm.sample(chains=1, draws=100000)\n",
    "\n",
    "print(model_no_transform.vars)\n",
    "print(f\"Diverging: {trace_no_transform.get_sampler_stats('diverging').sum()}\")\n",
    "```\n",
    "\n",
    "```none\n",
    "[sd ~ HalfNormal(sigma=10.0)]\n",
    "Diverging: 423\n",
    "```\n",
    "\n",
    "In the absence of automatic transforms the user would need to spend some\n",
    "time assessing why the divergences are occurring, and either know that a\n",
    "transformation is needed from prior experience or come to this\n",
    "conclusion through debugging and research, all efforts that take time\n",
    "away from building the model and performing inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwting/anaconda3/envs/rapids/lib/python3.11/site-packages/pymc/model/core.py:1395: UserWarning: To disable default transform, please use default_transform=None instead of transform=None. Setting transform to None will not have any effect in future.\n",
      "  warnings.warn(\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "NUTS: [sd]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d7bfb6c01d4dacb3fb1591cc9f544e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 1_000 tune and 100_000 draw iterations (1_000 + 100_000 draws total) took 13 seconds.\n",
      "There were 36 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{sd: sd, TensorConstant(TensorType(float64, shape=(20,)), data=array([-0. ... 01098095])): y}\n",
      "Diverging: 36\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as model_no_transform:\n",
    "    sd = pm.HalfNormal(\"sd\", 5, transform=None)\n",
    "    y = pm.Normal(\"y\", mu=0, sigma=sd, observed=y_observed)\n",
    "    idata_no_transform = pm.sample(chains=1, draws=100000)\n",
    "\n",
    "print(model_no_transform.values_to_rvs)\n",
    "print(f\"Diverging: {idata_no_transform.sample_stats['diverging'].sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(operation_graphs_ppl)=\n",
    "\n",
    "## 10.5. Operation Graphs and Automatic Reparameterization\n",
    "\n",
    "One manipulation that some PPLs perform is reparameterizing models, by\n",
    "first creating an *operation graph* and then subsequently optimizing\n",
    "that graph. To illustrate what this means let us define a computation:\n",
    "\n",
    "```{math} \n",
    ":label: eq:basic_arithmetic\n",
    "\\begin{split}\n",
    "    x=3 \\\\\n",
    "    y=1 \\\\\n",
    "    x*(y/x) + 0\n",
    "    \n",
    "\\end{split}\n",
    "```\n",
    "\n",
    "Humans, with basic algebra knowledge, will quickly see that the $x$\n",
    "terms cancel leaving the addition $y+0$, which has no effect, leading to\n",
    "an answer of 1. We can also perform this calculation in pure Python and\n",
    "get the same answer which is great, but what is not great is the wasted\n",
    "computation. Pure Python, and libraries like numpy, just see these\n",
    "operations as *computational steps* and will faithfully perform each\n",
    "step of the stated equation, first dividing $y$ by $x$, then multiplying\n",
    "that result by $x$, then adding 0.\n",
    "\n",
    "In contrast libraries like Theano work differently. They first construct\n",
    "a *symbolic* representation of the computation as shown in Code Block\n",
    "[unoptimized_symbolic_algebra](unoptimized_symbolic_algebra).\n",
    "\n",
    "\n",
    "```{code-block} python\n",
    ":name: unoptimized_symbolic_algebra\n",
    ":caption: unoptimized_symbolic_algebra\n",
    "\n",
    "x = theano.tensor.vector(\"x\")\n",
    "y = theano.tensor.vector(\"y\")\n",
    "out = x*(y/x) + 0\n",
    "theano.printing.debugprint(out)\n",
    "```\n",
    "\n",
    "```none\n",
    "Elemwise{add,no_inplace} [id A] ''   \n",
    " |Elemwise{mul,no_inplace} [id B] ''   \n",
    " | |x [id C]\n",
    " | |Elemwise{true_div,no_inplace} [id D] ''   \n",
    " |   |y [id E]\n",
    " |   |x [id C]\n",
    " |InplaceDimShuffle{x} [id F] ''   \n",
    "   |TensorConstant{0} [id G]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 3\n",
    "y = 1\n",
    "x * y / x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytensor.config.compute_test_value = 'ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add [id A]\n",
      " ├─ Mul [id B]\n",
      " │  ├─ x [id C]\n",
      " │  └─ True_div [id D]\n",
      " │     ├─ y [id E]\n",
      " │     └─ x [id C]\n",
      " └─ ExpandDims{axis=0} [id F]\n",
      "    └─ 0 [id G]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x705343616f50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pytensor.tensor.vector(\"x\")\n",
    "y = pytensor.tensor.vector(\"y\")\n",
    "out = x*(y/x) + 0\n",
    "pytensor.printing.debugprint(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {admonition} What is Aesara?\n",
    "\n",
    "As you have seen Theano is the workhorse of PyMC3 models\n",
    "in terms of graph representation, gradient calculation, and much more.\n",
    "However, Theano was deprecated in 2017 by the original authors. Since\n",
    "then PyMC developers have been maintaining Theano to support of PyMC3.\n",
    "In 2020 the PyMC developers decided to move from maintaining Theano to\n",
    "improving it. In doing so the PyMC developers forked Theano and named\n",
    "the fork Aesara [^12]. With a focused effort led by Brandon Willard the\n",
    "legacy portions of the code base have been drastically modernized.\n",
    "Additionally Aesara includes expanded functionality particularly for\n",
    "Bayesian use cases. These include adding new backends (JAX and Numba)\n",
    "for accelerated numerical computation and better support modern compute\n",
    "hardware such GPU and TPU. With greater control and coordination over\n",
    "more of the PPL components between the PyMC3 and Aesara the PyMC\n",
    "developers are looking to continuously foster a better PPL experience\n",
    "for developers, statisticians, and users.\n",
    ":::\n",
    "\n",
    "In the output of Code Block\n",
    "[unoptimized_symbolic_algebra](unoptimized_symbolic_algebra),\n",
    "working inside out, we see on Line 4 the first operation is the division\n",
    "of $x$ and $y$, then the multiplication $x$, then finally the addition\n",
    "of 0 represented in a computation graph. This same graph is shown\n",
    "visually in {numref}`fig:unoptimized_symbolic_algebra_graph`. At this\n",
    "point no actual numeric calculations have taken place, but a sequence of\n",
    "operations, albeit an unoptimized one, has been generated.\n",
    "\n",
    "```{figure} figures/symbolic_graph_unopt.png\n",
    ":name: fig:unoptimized_symbolic_algebra_graph\n",
    ":width: 8.00in\n",
    "Unoptimized Theano operation graph of Equation {eq}`eq:basic_arithmetic`\n",
    "as declared in Code Block\n",
    "[unoptimized_symbolic_algebra](unoptimized_symbolic_algebra).\n",
    "```\n",
    "\n",
    "We can now optimize this graph using Theano, by passing this computation\n",
    "graph to `theano.function` in Code Block\n",
    "[optimized_symbolic_algebra](optimized_symbolic_algebra).\n",
    "In the output nearly all the operations have disappeared, as Theano has\n",
    "recognized that both multiplication and division of $x$ cancels out, and\n",
    "that the addition of $0$ has no effect on the final outcome. The\n",
    "optimized operation graph is shown in\n",
    "{numref}`fig:optimized_symbolic_algebra`.\n",
    "\n",
    "\n",
    "```{code-block} python\n",
    ":name: optimized_symbolic_algebra\n",
    ":caption: optimized_symbolic_algebra\n",
    "\n",
    "fgraph = theano.function([x,y], [out])\n",
    "theano.printing.debugprint(fgraph)\n",
    "```\n",
    "\n",
    "```none\n",
    "DeepCopyOp [id A] 'y'   0\n",
    " |y [id B]\n",
    "```\n",
    "\n",
    "```{figure} figures/symbolic_graph_opt.png\n",
    ":name: fig:optimized_symbolic_algebra\n",
    ":width: 4.00in\n",
    "Optimized Theano operation graph of Equation {eq}`eq:basic_arithmetic`\n",
    "after optimization in\n",
    "[optimized_symbolic_algebra](optimized_symbolic_algebra).\n",
    "\n",
    "[^12]: In Greek mythology Aesara is the daughter of Theano, hence the\n",
    "    fitting name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCopyOp [id A] 0\n",
      " └─ y [id B]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x705343616f50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fgraph = pytensor.function([x,y], [out])\n",
    "pytensor.printing.debugprint(fgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano can then calculate the answer when the optimized function is\n",
    "called with the numerical inputs as shown in Code Block\n",
    "[optimized_symbolic_algebra_calc](optimized_symbolic_algebra_calc).\n",
    "\n",
    "```{code-block} python\n",
    ":name: optimized_symbolic_algebra_calc\n",
    ":caption: optimized_symbolic_algebra_calc\n",
    "\n",
    "fgraph([1],[3])\n",
    "```\n",
    "\n",
    "```none\n",
    "[array([3.])]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3.])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fgraph([1],[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is available at img/chp10/symbolic_graph_unopt.png\n",
      "The output file is available at img/chp10/symbolic_graph_opt.png\n"
     ]
    }
   ],
   "source": [
    "pytensor.printing.pydotprint(\n",
    "    out, outfile=\"img/chp10/symbolic_graph_unopt.png\",\n",
    "    var_with_name_simple=False, high_contrast=False, with_ids=True)\n",
    "pytensor.printing.pydotprint(\n",
    "    fgraph, \n",
    "    outfile=\"img/chp10/symbolic_graph_opt.png\", \n",
    "    var_with_name_simple=False, high_contrast=False, with_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the algebraic simplication, your computer did not become\n",
    "sentient and rederive the rules of algebra from scratch. Theano is able\n",
    "to perform these optimizations thanks to a code optimizer [^13] that\n",
    "inspects the operation graph stated through the Theano API by a user,\n",
    "scans the graph for algebraic patterns, and simplifies computation and\n",
    "give users the desired result.\n",
    "\n",
    "Bayesian models are just a special case of both mathematics and\n",
    "computation. In Bayesian computation typically desired output is the\n",
    "logp of the model. Before optimization the first step is a symbolic\n",
    "representation of the operations graph, an example of which is shown in\n",
    "Code Block [aesara_debug](aesara_debug) where a one line\n",
    "PyMC3 model is turned into multi line computation graph at the operation\n",
    "level.\n",
    "\n",
    "```{code-block} python\n",
    ":name: aesara_debug\n",
    ":caption: aesara_debug\n",
    "\n",
    "with pm.Model() as model_normal:\n",
    "    x = pm.Normal(\"x\", 0., 1.)\n",
    "\n",
    "theano.printing.debugprint(aesara_normal.logpt)\n",
    "```\n",
    "```none\n",
    "Sum{acc_dtype=float64} [id A] '__logp'   \n",
    " |MakeVector{dtype='float64'} [id B] ''   \n",
    "   |Sum{acc_dtype=float64} [id C] ''   \n",
    "     |Sum{acc_dtype=float64} [id D] '__logp_x'   \n",
    "       |Elemwise{switch,no_inplace} [id E] ''   \n",
    "         |Elemwise{mul,no_inplace} [id F] ''   \n",
    "         | |TensorConstant{1} [id G]\n",
    "         | |Elemwise{mul,no_inplace} [id H] ''   \n",
    "         |   |TensorConstant{1} [id I]\n",
    "         |   |Elemwise{gt,no_inplace} [id J] ''   \n",
    "         |     |TensorConstant{1.0} [id K]\n",
    "         |     |TensorConstant{0} [id L]\n",
    "         |Elemwise{true_div,no_inplace} [id M] ''   \n",
    "         | |Elemwise{add,no_inplace} [id N] ''   \n",
    "         | | |Elemwise{mul,no_inplace} [id O] ''   \n",
    "         | | | |Elemwise{neg,no_inplace} [id P] ''   \n",
    "         | | | | |TensorConstant{1.0} [id Q]\n",
    "         | | | |Elemwise{pow,no_inplace} [id R] ''   \n",
    "         | | |   |Elemwise{sub,no_inplace} [id S] ''   \n",
    "         | | |   | |x ~ Normal(mu=0.0, sigma=1.0) [id T]\n",
    "         | | |   | |TensorConstant{0.0} [id U]\n",
    "         | | |   |TensorConstant{2} [id V]\n",
    "         | | |Elemwise{log,no_inplace} [id W] ''   \n",
    "         | |   |Elemwise{true_div,no_inplace} [id X] ''   \n",
    "         | |     |Elemwise{true_div,no_inplace} [id Y] ''   \n",
    "         | |     | |TensorConstant{1.0} [id Q]\n",
    "         | |     | |TensorConstant{3.141592653589793} [id Z]\n",
    "         | |     |TensorConstant{2.0} [id BA]\n",
    "         | |TensorConstant{2.0} [id BB]\n",
    "         |TensorConstant{-inf} [id BC]\n",
    "```\n",
    "\n",
    "Just like algebraic optimization this graph can then be optimized in\n",
    "ways that benefit the Bayesian user {cite:p}`willard2020minikanren`. Recall the\n",
    "discussion in Section {ref}`model_geometry`, certain\n",
    "models benefit from non-centered parameterizations, as this helps\n",
    "eliminate challenging geometry such as Neal's funnel. Without automatic\n",
    "optimization the user must be aware of the geometrical challenge to the\n",
    "sampler and make the adjust themselves. In the future, libraries such as\n",
    "symbolic-pymc [^14] will be able to make this reparameterization\n",
    "automatic, just as we say the automatic transformation of log\n",
    "probability and bounded distributions above. With this upcoming tool PPL\n",
    "users can further focus on the model and let the PPL \"worry\\\" about the\n",
    "computational optimizations.\n",
    "\n",
    "[^13]: <https://theano-pymc.readthedocs.io/en/latest/optimizations.html?highlight=o1#optimizations>\n",
    "\n",
    "[^14]: <https://github.com/pymc-devs/symbolic-pymc>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pm.Model() as model_normal:\n",
    "#     x = pm.Normal(\"x\", 0., 1.)\n",
    "    \n",
    "# pytensor.printing.debugprint(model_normal.logp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(effect-handling)=\n",
    "\n",
    "## 10.6. Effect handling\n",
    "\n",
    "Effect handlers {cite:p}`kammar2013handlers` are an abstraction in programming\n",
    "languages that gives different interpretations, or side effects, to the\n",
    "standard behavior of statements in a program. A common example is\n",
    "exception handling in Python with `try` and `except`. When some specific\n",
    "error is raised in the code block under `try` statement, we can perform\n",
    "different processing in the `except` block and resume computation. For\n",
    "Bayesian models there are two primary effects we want the random\n",
    "variable to have, draw a value (sample) from its distribution, or\n",
    "condition the value to some user input. Other use cases of effect\n",
    "handlers are transforming bounded random variables and automatic\n",
    "reparameterization as we mentioned above.\n",
    "\n",
    "Effect handlers are not a required component of PPLs but rather a design\n",
    "choice that strongly influences the API and the \"feel\\\" of using the\n",
    "PPL. Harkening back to our car analogy this is similar to a power\n",
    "steering system in a car. It is not required, it is usually hidden from\n",
    "the driver under the hood but it definitely changes the driving\n",
    "experience. As effect handlers are typically \"hidden\\\" they are more\n",
    "easily explained through example rather than theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(example-effect-handling-in-tfp-and-numpyro)=\n",
    "\n",
    "### 10.6.1. Example: Effect Handling in TFP and Numpyro\n",
    "\n",
    "In the rest of this section we will see how effect handling works in\n",
    "TensorFlow Probability and NumPyro. Briefly NumPyro is another PPL based\n",
    "on Jax. Specifically, we will compare the high level API between\n",
    "`tfd.JointDistributionCoroutine` and model written with NumPyro\n",
    "primitives, which both represent Bayesian model with a Python function\n",
    "in a similar way. Also, we will be using the JAX substrate of TFP, so\n",
    "that both API share the same base language and numerical computation\n",
    "backend. Again consider the model in Equation\n",
    "{eq}`eq:simple_normal_model`, in Code Block\n",
    "[tfp_vs_numpyro](tfp_vs_numpyro) we import the libraries\n",
    "and write the model:\n",
    "\n",
    "```{code-block} python\n",
    ":name: tfp_vs_numpyro\n",
    ":caption: tfp_vs_numpyro\n",
    "\n",
    "import jax\n",
    "import numpyro\n",
    "from tensorflow_probability.substrates import jax as tfp_jax\n",
    "\n",
    "tfp_dist = tfp_jax.distributions\n",
    "numpyro_dist = numpyro.distributions\n",
    "\n",
    "root = tfp_dist.JointDistributionCoroutine.Root\n",
    "def tfp_model():\n",
    "    x = yield root(tfp_dist.Normal(loc=1.0, scale=2.0, name=\"x\"))\n",
    "    z = yield root(tfp_dist.HalfNormal(scale=1., name=\"z\"))\n",
    "    y = yield tfp_dist.Normal(loc=x, scale=z, name=\"y\")\n",
    "    \n",
    "def numpyro_model():\n",
    "    x = numpyro.sample(\"x\", numpyro_dist.Normal(loc=1.0, scale=2.0))\n",
    "    z = numpyro.sample(\"z\", numpyro_dist.HalfNormal(scale=1.0))\n",
    "    y = numpyro.sample(\"y\", numpyro_dist.Normal(loc=x, scale=z))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpyro\n",
    "from tensorflow_probability.substrates import jax as tfp_jax\n",
    "\n",
    "tfp_dist = tfp_jax.distributions\n",
    "numpyro_dist = numpyro.distributions\n",
    "\n",
    "root = tfp_dist.JointDistributionCoroutine.Root\n",
    "def tfp_model():\n",
    "    x = yield root(tfp_dist.Normal(loc=1.0, scale=2.0, name=\"x\"))\n",
    "    z = yield root(tfp_dist.HalfNormal(scale=1., name=\"z\"))\n",
    "    y = yield tfp_dist.Normal(loc=x, scale=z, name=\"y\")\n",
    "    \n",
    "def numpyro_model():\n",
    "    x = numpyro.sample(\"x\", numpyro_dist.Normal(loc=1.0, scale=2.0))\n",
    "    z = numpyro.sample(\"z\", numpyro_dist.HalfNormal(scale=1.0))\n",
    "    y = numpyro.sample(\"y\", numpyro_dist.Normal(loc=x, scale=z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object tfp_model at 0x70517a1e7540>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(tfp_model())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(numpyro_model())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a glance, `tfp_model` and `numpyro_model` looks similar, both are\n",
    "Python functions with no input argument and return statement (note\n",
    "NumPyro model can have inputs and return statements), both need to\n",
    "indicate which statement should be considered as random variable (TFP\n",
    "with `yield`, NumPyro with `numpyro.sample` primitives). Moreover, the\n",
    "default behavior of both `tfp_model` and `numpyro_model` is ambiguous,\n",
    "they do not really do anything [^15] until you give it specific\n",
    "instruction. For example, in Code Block\n",
    "[tfp_vs_numpyro_prior_sample](tfp_vs_numpyro_prior_sample)\n",
    "we draw prior samples from both models, and evaluate the log probability\n",
    "on the same prior samples (that returned by the TFP model).\n",
    "\n",
    "```{code-block} python\n",
    ":name: tfp_vs_numpyro_prior_sample\n",
    ":caption: tfp_vs_numpyro_prior_sample\n",
    "\n",
    "sample_key = jax.random.PRNGKey(52346)\n",
    "\n",
    "# Draw samples\n",
    "jd = tfp_dist.JointDistributionCoroutine(tfp_model)\n",
    "tfp_sample = jd.sample(1, seed=sample_key)\n",
    "\n",
    "predictive = numpyro.infer.Predictive(numpyro_model, num_samples=1)\n",
    "numpyro_sample = predictive(sample_key)\n",
    "\n",
    "# Evaluate log prob\n",
    "log_likelihood_tfp = jd.log_prob(tfp_sample)\n",
    "log_likelihood_numpyro = numpyro.infer.util.log_density(\n",
    "    numpyro_model, [], {},\n",
    "    # Samples returning from JointDistributionCoroutine is a\n",
    "    # Namedtuple like Python object, we convert it to a dictionary\n",
    "    # so that numpyro can recognize it.\n",
    "    params=tfp_sample._asdict())\n",
    "\n",
    "# Validate that we get the same log prob\n",
    "np.testing.assert_allclose(log_likelihood_tfp, log_likelihood_numpyro[0])\n",
    "```\n",
    "\n",
    "[^15]: The default behavior of a Pyro model is to sample from the\n",
    "    distribution, but not in NumPyro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key = jax.random.PRNGKey(52346)\n",
    "\n",
    "# Draw samples\n",
    "jd = tfp_dist.JointDistributionCoroutine(tfp_model)\n",
    "tfp_sample = jd.sample(1, seed=sample_key)\n",
    "\n",
    "predictive = numpyro.infer.Predictive(numpyro_model, num_samples=1)\n",
    "numpyro_sample = predictive(sample_key)\n",
    "\n",
    "# Evaluate log prob\n",
    "log_likelihood_tfp = jd.log_prob(tfp_sample)\n",
    "log_likelihood_numpyro = numpyro.infer.util.log_density(\n",
    "    numpyro_model, [], {},\n",
    "    # Samples returning from JointDistributionCoroutine is a\n",
    "    # Namedtuple like Python object, we convert it to a dictionary\n",
    "    # so that numpyro can recognize it.\n",
    "    params=tfp_sample._asdict())\n",
    "\n",
    "# Validate that we get the same log prob\n",
    "np.testing.assert_allclose(log_likelihood_tfp, log_likelihood_numpyro[0], rtol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also condition some random variable to user input value in our\n",
    "model, for example, in Code Block\n",
    "[tfp_vs_numpyro_condition](tfp_vs_numpyro_condition) we\n",
    "condition `z = .01` and then sample from the model.\n",
    "\n",
    "\n",
    "```{code-block} python\n",
    ":name: tfp_vs_numpyro_condition\n",
    ":caption: tfp_vs_numpyro_condition\n",
    "\n",
    "# Condition z to .01 in TFP and sample\n",
    "jd.sample(z=.01, seed=sample_key)\n",
    "\n",
    "# Condition z to .01 in NumPyro and sample\n",
    "predictive = numpyro.infer.Predictive(\n",
    "    numpyro_model, num_samples=1, params={\"z\": np.asarray(.01)})\n",
    "predictive(sample_key)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructTuple(\n",
       "  x=Array(0.4250738, dtype=float32),\n",
       "  z=Array(0.01, dtype=float32),\n",
       "  y=Array(0.42213488, dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Condition z to .01 in TFP and sample\n",
    "jd.sample(z=.01, seed=sample_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': Array([1.1232406], dtype=float32),\n",
       " 'y': Array([1.1318898], dtype=float32),\n",
       " 'z': array([0.01])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Condition z to .01 in NumPyro and sample\n",
    "predictive = numpyro.infer.Predictive(\n",
    "    numpyro_model, num_samples=1, params={\"z\": np.asarray(.01)})\n",
    "predictive(sample_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From user perspective effect handling mostly happens behind the scenes\n",
    "when high level APIs are used. In TFP a `tfd.JointDistribution`\n",
    "encapsulate the effect handlers inside of a single object, and change\n",
    "the behavior of a function within that object when the input arguments\n",
    "are different. For NumPyro the effect handling is a bit more explicit\n",
    "and flexible. A set of effect handlers are implemented in\n",
    "`numpyro.handlers`, which powers the high level APIs we just used to\n",
    "generate prior samples and compute model log probability. This is shown\n",
    "again in Code Block\n",
    "[tfp_vs_numpyro_condition_distribution](tfp_vs_numpyro_condition_distribution),\n",
    "where we conditioned random variable $z = .01$, draw a sample from $x$,\n",
    "and construct conditional distribution $p(y \\mid x, z)$ and sample from\n",
    "it.\n",
    "\n",
    "```{code-block} python\n",
    ":name: tfp_vs_numpyro_condition_distribution\n",
    ":caption: tfp_vs_numpyro_condition_distribution\n",
    "\n",
    "# Conditioned z to .01 in TFP and construct conditional distributions\n",
    "dist, value = jd.sample_distributions(z=.01, seed=sample_key)\n",
    "assert dist.y.loc == value.x\n",
    "assert dist.y.scale == value.z\n",
    "\n",
    "# Conditioned z to .01 in NumPyro and construct conditional distributions\n",
    "model = numpyro.handlers.substitute(numpyro_model, data={\"z\": .01})\n",
    "with numpyro.handlers.seed(rng_seed=sample_key):\n",
    "    # Under the seed context, the default behavior of a NumPyro model is the\n",
    "    # same as in Pyro: drawing prior sample.\n",
    "    model_trace = numpyro.handlers.trace(numpyro_model).get_trace()\n",
    "assert model_trace[\"y\"][\"fn\"].loc == model_trace[\"x\"][\"value\"]\n",
    "assert model_trace[\"y\"][\"fn\"].scale == model_trace[\"z\"][\"value\"]\n",
    "```\n",
    "\n",
    "The Python assertion in Code Block\n",
    "[tfp_vs_numpyro_condition_distribution](tfp_vs_numpyro_condition_distribution)\n",
    "is to validate that the conditional distribution is indeed correct.\n",
    "Compare to the `jd.sample_distributions(.)` call, You could see the\n",
    "explicit effect handling in NumPyro with `numpyro.handlers.substitute`\n",
    "that returns a conditioned model, `numpyro.handlers.seed` to set the\n",
    "random seed (a JAX requirement for drawing random samples), and\n",
    "`numpyro.handlers.trace` to trace the function execution. More\n",
    "information of the effect handling in NumPyro and Pyro could be found in\n",
    "their official documentation [^16].\n",
    "\n",
    "[^16]: <https://pyro.ai/examples/effect_handlers.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditioned z to .01 in TFP and construct conditional distributions\n",
    "dist, value = jd.sample_distributions(z=.01, seed=sample_key)\n",
    "assert dist.y.loc == value.x\n",
    "assert dist.y.scale == value.z\n",
    "\n",
    "# Conditioned z to .01 in NumPyro and construct conditional distributions\n",
    "model = numpyro.handlers.substitute(numpyro_model, data={\"z\": .01})\n",
    "with numpyro.handlers.seed(rng_seed=sample_key):\n",
    "    # Under the seed context, the default behavior of a NumPyro model is the\n",
    "    # same as in Pyro: drawing prior sample.\n",
    "    model_trace = numpyro.handlers.trace(numpyro_model).get_trace()\n",
    "assert model_trace[\"y\"][\"fn\"].loc == model_trace[\"x\"][\"value\"]\n",
    "assert model_trace[\"y\"][\"fn\"].scale == model_trace[\"z\"][\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(base-language-code-ecosystem-modularity-and-everything-else)=\n",
    "\n",
    "## 10.7. Base Language, Code Ecosystem, Modularity and Everything Else\n",
    "\n",
    "When serious car enthusiasts pick a car, the availability of different\n",
    "components that can be mixed and match can be an informative factor in\n",
    "which car is ultimately purchased. These owners may choose to make\n",
    "aesthetic changes to fit their preference such as a new hood for a\n",
    "different look, or they may choose to perform an engine swap, which\n",
    "substantially changes the performance of the vehicle. Regardless most\n",
    "car owners would prefer to have more choices and flexibility in how they\n",
    "can modify their vehicle then less, even if they do not choose to modify\n",
    "their vehicle at all.\n",
    "\n",
    "In this same way PPL users are not only concerned about the PPL itself,\n",
    "but also what related code bases and packages exist in that particular\n",
    "ecosystem, as well as the modularity of the PPL itself. In this book we\n",
    "have used Python as the base language, and PyMC3 and TensorFlow\n",
    "Probability as our PPLs. With them however, we have also used Matplotlib\n",
    "for plotting, NumPy for numerical operations, Pandas and xarray for data\n",
    "manipulation, and ArviZ for exploratory analysis of Bayesian models.\n",
    "Colloquially these are all part of the PyData stack. However, there are\n",
    "other base languages such as R with their own ecosystem of packages.\n",
    "This ecosystem has similar set of tools under the tidyverse moniker, as\n",
    "well as specific Bayesian packages aptly named loo, posterior, bayesplot\n",
    "among others. Luckily Stan users are able to change base languages\n",
    "relatively easily, as the model is defined in the Stan language and\n",
    "there is a choice of interfaces available such as pystan, rstan, cmdstan\n",
    "and others. PyMC3 users are relegated to Python. However, with Theano\n",
    "there is modularity in the computational backend that can be used, from\n",
    "the Theano native backend, to the newer JAX backend. Along with all the\n",
    "above there is a laundry list of other points that matter,\n",
    "non-uniformly, to PPL users including.\n",
    "\n",
    "-   Ease of development in production environments\n",
    "\n",
    "-   Ease of installation in development environment\n",
    "\n",
    "-   Developer speed\n",
    "\n",
    "-   Computational speed\n",
    "\n",
    "-   Availability of papers, blog posts, lectures\n",
    "\n",
    "-   Documentation\n",
    "\n",
    "-   Useful error messages\n",
    "\n",
    "-   The community\n",
    "\n",
    "-   What colleagues recommend\n",
    "\n",
    "-   Upcoming features\n",
    "\n",
    "Just having choices is not enough however, to use a PPL a user must be\n",
    "able to install and understand how to use them. The availability of work\n",
    "that references the PPL tends to indicate how widely accepted it is and\n",
    "provide confidence that it is indeed useful. Users are not keen to\n",
    "invest time into a PPL that will no longer be maintained. And ultimately\n",
    "as humans, even data informed Bayesian users, the recommendation of a\n",
    "respected colleagues and other presence of a large user base, are all\n",
    "influential factors in evaluating PPL, as much as the technical\n",
    "capabilities in many circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(designing-a-ppl)=\n",
    "\n",
    "## 10.8. Designing a PPL\n",
    "\n",
    "In this section we will switch our perspective from a PPL overview as a\n",
    "user and to one of a PPL designer. Now that we identified the big\n",
    "components let us design a hypothetical PPL to see how components fit\n",
    "together and also how they sometimes do not fit as easily as you would\n",
    "hope! The choices we will make are for illustrative purposes but frame\n",
    "how the system comes together, and also how a PPL designer thinks when\n",
    "putting together a PPL.\n",
    "\n",
    "First we choose a base language with a numerical computing backend.\n",
    "Since this book focuses on Python let us use NumPy. Ideally, we also\n",
    "have a set of commonly used mathematical functions implemented for us\n",
    "already. For example, the central piece for implementing a PPL is a set\n",
    "of (log)probability mass or density function, and some pseudo random\n",
    "number generators. Luckily, those are readily available via\n",
    "`scipy.stats`. Let us put these together in Code Block\n",
    "[scipy_stats](scipy_stats) with a simple demonstration of\n",
    "drawing some samples from a $\\mathcal{N}(1, 2)$ distribution and\n",
    "evaluate their log probability:\n",
    "\n",
    "\n",
    "```{code-block} python\n",
    ":name: scipy_stats\n",
    ":caption: scipy_stats\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Draw 2 samples from a Normal(1., 2.) distribution\n",
    "x = stats.norm.rvs(loc=1.0, scale=2.0, size=2, random_state=1234)\n",
    "# Evaluate the log probability of the samples \n",
    "logp = stats.norm.logpdf(x, loc=1.0, scale=2.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Draw 2 samples from a Normal(1., 2.) distribution\n",
    "x = stats.norm.rvs(loc=1.0, scale=2.0, size=2, random_state=1234)\n",
    "# Evaluate the log probability of the samples \n",
    "logp = stats.norm.logpdf(x, loc=1.0, scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `stats.norm` is a Python class in the `scipy.stats` module [^17]\n",
    "which contains methods and statistical functions associated with *the\n",
    "family of* Normal distributions. Alternatively, we can initialize a\n",
    "Normal distribution with fixed parameters as shown in Code Block\n",
    "[scipy_stats2](scipy_stats2).\n",
    "\n",
    "\n",
    "```{code-block} python\n",
    ":name: scipy_stats2\n",
    ":caption: scipy_stats2\n",
    "random_variable_x = stats.norm(loc=1.0, scale=2.0)\n",
    "\n",
    "x = random_variable_x.rvs(size=2, random_state=1234)\n",
    "logp = random_variable_x.logpdf(x)\n",
    "```\n",
    "\n",
    "[^17]: <https://docs.scipy.org/doc/scipy/reference/stats.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_variable_x = stats.norm(loc=1.0, scale=2.0)\n",
    "\n",
    "x = random_variable_x.rvs(size=2, random_state=1234)\n",
    "logp = random_variable_x.logpdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Code Blocks [scipy_stats](scipy_stats) and\n",
    "[scipy_stats2](scipy_stats2) return exactly the same\n",
    "output `x` and `logp` as we also supplied the same `random_state`. The\n",
    "differences here is that Code Block\n",
    "[scipy_stats2](scipy_stats2) we have a \"frozen\\\" random\n",
    "variable [^18] `random_variable_x` that could be considered as the SciPy\n",
    "representation of $x \\sim \\mathcal{N}(1, 2)$. Unfortunately, this object\n",
    "does work well when we try to use it naively when writing a full\n",
    "Bayesian models. Consider the model\n",
    "$x \\sim \\mathcal{N}(1, 2), y \\sim \\mathcal{N}(x, 0.1)$. Writing it in\n",
    "Code Block\n",
    "[simple_model_not_working_scipy](simple_model_not_working_scipy)\n",
    "raises an exception because `scipy.stats.norm` is expecting the input to\n",
    "be a NumPy array [^19].\n",
    "\n",
    "```{code-block} python\n",
    ":name: simple_model_not_working_scipy\n",
    ":caption: simple_model_not_working_scipy\n",
    "\n",
    "x = stats.norm(loc=1.0, scale=2.0)\n",
    "y = stats.norm(loc=x, scale=0.1)\n",
    "y.rvs()\n",
    "```\n",
    "\n",
    "```none\n",
    "...\n",
    "TypeError: unsupported operand type(s) for +: 'float' and 'rv_frozen'\n",
    "```\n",
    "\n",
    "[^18]: We will go into more details about random variable in Chapter\n",
    "    [11](app).\n",
    "\n",
    "[^19]: To be more precise, a Python object with a `__array__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'rv_continuous_frozen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mnorm(loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mnorm(loc\u001b[38;5;241m=\u001b[39mx, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:491\u001b[0m, in \u001b[0;36mrv_frozen.rvs\u001b[0;34m(self, size, random_state)\u001b[0m\n\u001b[1;32m    489\u001b[0m kwds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    490\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m: size, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: random_state})\n\u001b[0;32m--> 491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:1069\u001b[0m, in \u001b[0;36mrv_generic.rvs\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state\n\u001b[1;32m   1067\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rvs(\u001b[38;5;241m*\u001b[39margs, size\u001b[38;5;241m=\u001b[39msize, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 1069\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[43mvals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;66;03m# do not forget to restore the _random_state\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rndm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'rv_continuous_frozen'"
     ]
    }
   ],
   "source": [
    "x = stats.norm(loc=1.0, scale=2.0)\n",
    "y = stats.norm(loc=x, scale=0.1)\n",
    "y.rvs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this it becomes evident how tricky it is to design an API, what\n",
    "seems intuitive for the user may not be possible with the underlying\n",
    "packages, In our case to write a PPL in Python we need to make a series\n",
    "of API design choices and other decision to make Code Block\n",
    "[simple_model_not_working_scipy](simple_model_not_working_scipy)\n",
    "work. Specifically we want:\n",
    "\n",
    "1.  A representation of random variables that could be used to\n",
    "    initialize another random variable;\n",
    "\n",
    "2.  To be able to condition the random variable on some specific values\n",
    "    (e.g., the observed data);\n",
    "\n",
    "3.  The graphical model, generated by a collection of random variables,\n",
    "    to behave in a consistent and predictable way.\n",
    "\n",
    "Getting Item 1 to work is actually pretty straightforward with a Python\n",
    "class that could be recognized by NumPy as an array. We do this in Code\n",
    "Block [scipy_rv0](scipy_rv0) and use the implementation to\n",
    "specific the model in Equation {eq}`eq:simple_normal_model`.\n",
    "\n",
    "```{math} \n",
    ":label: eq:simple_normal_model\n",
    "\n",
    "\\begin{split}\n",
    "    x \\sim& \\mathcal{N}(1, 2) \\\\\n",
    "    z \\sim& \\mathcal{HN}(1) \\\\\n",
    "    y \\sim& \\mathcal{N}(x, z)\n",
    "\\end{split}\n",
    "```\n",
    "\n",
    "```{code-block} python\n",
    ":name: scipy_rv0\n",
    ":caption: scipy_rv0\n",
    "\n",
    "class RandomVariable:\n",
    "    def __init__(self, distribution):\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def __array__(self):\n",
    "        return np.asarray(self.distribution.rvs())\n",
    "\n",
    "x = RandomVariable(stats.norm(loc=1.0, scale=2.0))\n",
    "z = RandomVariable(stats.halfnorm(loc=0., scale=1.))\n",
    "y = RandomVariable(stats.norm(loc=x, scale=z))\n",
    "\n",
    "for i in range(5):\n",
    "    print(np.asarray(y))\n",
    "```\n",
    "\n",
    "```none\n",
    "3.7362186279475353\n",
    "0.5877468494932253\n",
    "4.916129854385227\n",
    "1.7421638350544257\n",
    "2.074813968631388\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1185728014575584\n",
      "-1.0685733280136258\n",
      "0.2884862400964521\n",
      "1.5133930997934741\n",
      "1.1682777240967406\n"
     ]
    }
   ],
   "source": [
    "class RandomVariable:\n",
    "    def __init__(self, distribution):\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def __array__(self):\n",
    "        return np.asarray(self.distribution.rvs())\n",
    "\n",
    "x = RandomVariable(stats.norm(loc=1.0, scale=2.0))\n",
    "z = RandomVariable(stats.halfnorm(loc=0., scale=1.))\n",
    "y = RandomVariable(stats.norm(loc=x, scale=z))\n",
    "\n",
    "for i in range(5):\n",
    "    print(np.asarray(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more precise description for the Python class we wrote in Code Block\n",
    "[scipy_rv0](scipy_rv0) is a stochastic array. As you see\n",
    "from the Code Block output, instantiation of this object, like\n",
    "`np.asarray(y)`, always gives us a different array. Adding a method to\n",
    "conditioned the random variable to some value, with a `log_prob` method,\n",
    "we have in Code Block [scipy_rv1](scipy_rv1) a toy\n",
    "implementation of a more functional `RandomVariable`:\n",
    "\n",
    "```{code-block} python\n",
    ":name: scipy_rv1\n",
    ":caption: scipy_rv1\n",
    "\n",
    "class RandomVariable:\n",
    "    def __init__(self, distribution, value=None):\n",
    "        self.distribution = distribution\n",
    "        self.set_value(value)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(value={self.__array__()})\"\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        if self.value is None:\n",
    "            return np.asarray(self.distribution.rvs(), dtype=dtype)\n",
    "        return self.value\n",
    "\n",
    "    def set_value(self, value=None):\n",
    "        self.value = value\n",
    "    \n",
    "    def log_prob(self, value=None):\n",
    "        if value is not None:\n",
    "            self.set_value(value)\n",
    "        return self.distribution.logpdf(np.array(self))\n",
    "\n",
    "x = RandomVariable(stats.norm(loc=1.0, scale=2.0))\n",
    "z = RandomVariable(stats.halfnorm(loc=0., scale=1.))\n",
    "y = RandomVariable(stats.norm(loc=x, scale=z))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomVariable:\n",
    "    def __init__(self, distribution, value=None):\n",
    "        self.distribution = distribution\n",
    "        self.set_value(value)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}(value={self.__array__()})\"\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        if self.value is None:\n",
    "            return np.asarray(self.distribution.rvs(), dtype=dtype)\n",
    "        return self.value\n",
    "\n",
    "    def set_value(self, value=None):\n",
    "        self.value = value\n",
    "    \n",
    "    def log_prob(self, value=None):\n",
    "        if value is not None:\n",
    "            self.set_value(value)\n",
    "        return self.distribution.logpdf(np.array(self))\n",
    "\n",
    "x = RandomVariable(stats.norm(loc=1.0, scale=2.0))\n",
    "z = RandomVariable(stats.halfnorm(loc=0., scale=1.))\n",
    "y = RandomVariable(stats.norm(loc=x, scale=z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the value of `y` with or without conditioning the value\n",
    "of its dependencies in Code Block\n",
    "[scipy_rv1_value](scipy_rv1_value), and the output seems\n",
    "to match the expected behavior. In Code Block below note how `y` is much\n",
    "closer to `x` if we set `z` to a small value.\n",
    "\n",
    "```{code-block} python\n",
    ":name: scipy_rv1_value\n",
    ":caption: scipy_rv1_value\n",
    "\n",
    "for i in range(3):\n",
    "    print(y)\n",
    "\n",
    "print(f\"  Set x=5 and z=0.1\")\n",
    "x.set_value(np.asarray(5))\n",
    "z.set_value(np.asarray(0.05))\n",
    "for i in range(3):\n",
    "    print(y)\n",
    "\n",
    "print(f\"  Reset z\")\n",
    "z.set_value(None)\n",
    "for i in range(3):\n",
    "    print(y)\n",
    "```\n",
    "\n",
    "```none\n",
    "RandomVariable(value=5.044294197842362)\n",
    "RandomVariable(value=4.907595148778454)\n",
    "RandomVariable(value=6.374656988711546)\n",
    "  Set x=5 and z=0.1\n",
    "RandomVariable(value=4.973898547458924)\n",
    "RandomVariable(value=4.959593974224869)\n",
    "RandomVariable(value=5.003811456458226)\n",
    "  Reset z\n",
    "RandomVariable(value=6.421473681641824)\n",
    "RandomVariable(value=4.942894375257069)\n",
    "RandomVariable(value=4.996621204780431)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomVariable(value=1.2629622385701529)\n",
      "RandomVariable(value=2.869579196206626)\n",
      "RandomVariable(value=-0.15541856655512165)\n",
      "  Set x=5 and z=0.1\n",
      "RandomVariable(value=5.081546715732569)\n",
      "RandomVariable(value=4.995970326035102)\n",
      "RandomVariable(value=5.007040485767257)\n",
      "  Reset z\n",
      "RandomVariable(value=5.630686177686023)\n",
      "RandomVariable(value=6.595375112516783)\n",
      "RandomVariable(value=4.408163913812902)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(y)\n",
    "\n",
    "print(f\"  Set x=5 and z=0.1\")\n",
    "x.set_value(np.asarray(5))\n",
    "z.set_value(np.asarray(0.05))\n",
    "for i in range(3):\n",
    "    print(y)\n",
    "\n",
    "print(f\"  Reset z\")\n",
    "z.set_value(None)\n",
    "for i in range(3):\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we can evaluate the unnormalized log probability density of\n",
    "the random variable. For example, in Code Block\n",
    "[scipy_rv1_posterior](scipy_rv1_posterior) we generate the\n",
    "posterior distribution for `x` and `z` when we observe `y = 5.0`.\n",
    "\n",
    "```{code-block} python\n",
    ":name: scipy_rv1_posterior\n",
    ":caption: scipy_rv1_posterior\n",
    "\n",
    "# Observed y = 5.\n",
    "y.set_value(np.array(5.))\n",
    "\n",
    "posterior_density = lambda xval, zval: x.log_prob(xval) + z.log_prob(zval) + y.log_prob()\n",
    "posterior_density(np.array(0.), np.array(1.))\n",
    "```\n",
    "\n",
    "```none\n",
    "-15.881815599614018\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-15.881815599614018"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observed y = 5.\n",
    "y.set_value(np.array(5.))\n",
    "\n",
    "posterior_density = lambda xval, zval: x.log_prob(xval) + z.log_prob(zval) + \\\n",
    "                y.log_prob()\n",
    "posterior_density(np.array(0.), np.array(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can validate it with an explicit implementation of the posterior\n",
    "density function, as shown in Code Block\n",
    "[scipy_posterior](scipy_posterior):\n",
    "\n",
    "```{code-block} python\n",
    ":name: scipy_posterior\n",
    ":caption: scipy_posterior\n",
    "\n",
    "def log_prob(xval, zval, yval=5):\n",
    "    x_dist = stats.norm(loc=1.0, scale=2.0)\n",
    "    z_dist = stats.halfnorm(loc=0., scale=1.)\n",
    "    y_dist = stats.norm(loc=xval, scale=zval)\n",
    "    return x_dist.logpdf(xval) + z_dist.logpdf(zval) + y_dist.logpdf(yval)\n",
    "\n",
    "log_prob(0, 1)\n",
    "```\n",
    "\n",
    "```none\n",
    "-15.881815599614018\n",
    "\n",
    "\n",
    "```python\n",
    "def log_prob(xval, zval, yval=5):\n",
    "    x_dist = stats.norm(loc=1.0, scale=2.0)\n",
    "    z_dist = stats.halfnorm(loc=0., scale=1.)\n",
    "    y_dist = stats.norm(loc=xval, scale=zval)\n",
    "    return x_dist.logpdf(xval) + z_dist.logpdf(zval) + y_dist.logpdf(yval)\n",
    "\n",
    "log_prob(0, 1)\n",
    "```\n",
    "\n",
    "At this point, it seems we have fulfilled the requirements of Item 1 and\n",
    "Item 2 , but Item 3 is the most challenging [^20]. For example, in a\n",
    "Bayesian workflow we want to draw prior and prior predictive sample from\n",
    "a model. While our `RandomVariable` draws a random sample according to\n",
    "its prior, when it is not conditioned on some value, it does not record\n",
    "the values of its parents (in a graphical model sense). We need\n",
    "additional graph utilities assign to `RandomVariable` so that the Python\n",
    "object aware of its parents and children (i.e., its Markov blanket), and\n",
    "propagates the change accordingly if we draw a new sample or conditioned\n",
    "on some specific value [^21]. For example, PyMC3 uses Theano to\n",
    "represent the graphical model and keep track of the dependencies (see Section\n",
    "{ref}`operation_graphs_ppl` above) and Edward [^22] uses TensorFlow v1\n",
    "[^23] to achieve that.\n",
    "\n",
    "::: {admonition} Spectrum of Probabilistic Modelling Libraries\n",
    "\n",
    "One aspect of PPLs that is worth mentioning is universality. A universal PPL is a PPL that is\n",
    "**Turing-complete**. Since the PPLs used in this book are an extension\n",
    "of a general-purpose base language, they could all be considered\n",
    "Turing-complete. However, research and implementation dedicated to\n",
    "universal PPLs usually focus on areas slightly different from what we\n",
    "discussed here. For example, an area of focus in a universal PPL is to\n",
    "express dynamic models, where the model contains complex control flow\n",
    "that dependent on random variable {cite:p}`wood2014new`. As a result, the\n",
    "number of random variables or the shape of a random variable could\n",
    "change during the execution of a dynamic probabilistic model. A good\n",
    "example of universal PPLs is Anglican {cite:p}`tolpin2016design`. Dynamic\n",
    "models might be valid or possible to write down, but there might not be\n",
    "an efficient and robust method to inference them. In this book, we\n",
    "discuss mainly PPLs focusing on static models (and their inference),\n",
    "with a slight sacrifice and neglect of universality. On the other end of\n",
    "the spectrum of universality, there are great software libraries that\n",
    "focus on some specific probabilistic models and their specialized\n",
    "inference [^24], which could be better suited for user's applications\n",
    "and use cases.\n",
    ":::\n",
    "\n",
    "Another approach is to treat model in a more encapsulated way and write\n",
    "the model as a Python function. Code Block\n",
    "[scipy_posterior](scipy_posterior) gave an example\n",
    "implementation of the joint log probability density function of the\n",
    "model in Equation {eq}`eq:simple_normal_model`, but for prior samples we\n",
    "need to again rewrite it a bit, shown in Code Block\n",
    "[scipy_prior](scipy_prior):\n",
    "\n",
    "```{code-block} python\n",
    ":name: scipy_prior\n",
    ":caption: scipy_prior\n",
    "\n",
    "def prior_sample():\n",
    "    x = stats.norm(loc=1.0, scale=2.0).rvs()\n",
    "    z = stats.halfnorm(loc=0., scale=1.).rvs()\n",
    "    y = stats.norm(loc=x, scale=z).rvs()\n",
    "    return x, z, y\n",
    "```\n",
    "\n",
    "With effect handling and function tracing [^25] in Python, we can\n",
    "actually combine `log_prob` from Code Block\n",
    "[scipy_posterior](scipy_posterior) and `sample` from Code\n",
    "Block [scipy_prior](scipy_prior) into a single Python\n",
    "function the user just need to write once. The PPL will then change the\n",
    "behavior of how the function is executed depending on the context\n",
    "(whether we are trying to obtain prior samples or evaluate the log\n",
    "probability). This approach of writing a Bayesian model as function and\n",
    "apply effect handler has gained significant popularity in recent years\n",
    "with Pyro {cite:p}`bingham2019pyro` (and NumPyro {cite:p}`phan2019composable`),\n",
    "Edward2 {cite:p}`tran2018simple, moore2018effect`, and JointDistribution in\n",
    "TensorFlow Probability {cite:p}`piponi2020joint` [^26] [^27].\n",
    "\n",
    "[^20]: Getting the shape right, minimizing unwanted side effects, to\n",
    "    name a few.\n",
    "\n",
    "[^21]: The graphical representation of a Bayesian Model is a central\n",
    "    concept in PPL, but in many cases they are implicit.\n",
    "\n",
    "[^22]: <https://github.com/blei-lab/edward>\n",
    "\n",
    "[^23]: The API of TensorFlow changed significantly between v1 and the\n",
    "    current version (v2).\n",
    "\n",
    "[^24]: For example, <https://github.com/jmschrei/pomegranate> for\n",
    "    Bayesian Network.\n",
    "\n",
    "[^25]: See the Python documentation for a complete explanation\n",
    "    <https://docs.python.org/3/library/trace.html>\n",
    "\n",
    "[^26]: Also see mcx <https://github.com/rlouf/mcx> that use Python AST\n",
    "    to do function rewrite; and oryx\n",
    "    <https://www.tensorflow.org/probability/oryx> that make use of the\n",
    "    JAX tracing for function transformation\n",
    "\n",
    "[^27]: If you are interested in more details about PPL development in\n",
    "    Python, take a look at this PyData Talk:\n",
    "    <https://www.youtube.com/watch?v=WHoS1ETYFrw>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6046584090424669, 0.44099902346558, -0.7914929873697657)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prior_sample():\n",
    "    x = stats.norm(loc=1.0, scale=2.0).rvs()\n",
    "    z = stats.halfnorm(loc=0., scale=1.).rvs()\n",
    "    y = stats.norm(loc=x, scale=z).rvs()\n",
    "    return x, z, y\n",
    "\n",
    "prior_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(shape_ppl)=\n",
    "\n",
    "### 10.8.1. Shape Handling in PPLs\n",
    "\n",
    "Something that all PPLs must deal with, and subsquently PPL designers\n",
    "must think about, is shapes. One of the common requests for help and\n",
    "frustrations that PPL designers hear from Bayesian modeler and\n",
    "practitioner are about *shape errors*. They are misspecification of the\n",
    "intended flow of array computation, which can cause issues like\n",
    "broadcasting errors. In this section we will give some examples to\n",
    "highly some subtleties of shape handling in PPLs.\n",
    "\n",
    "Consider the prior predictive sample function defined in Code Block\n",
    "[scipy_prior](scipy_prior) for the model in Equation\n",
    "{eq}`eq:simple_normal_model`, executing the function draws a single\n",
    "sample from the prior and prior predictive distribution, it is certainly\n",
    "quite inefficient if we want to draw a large among of iid samples from\n",
    "it. Distribution in `scipy.stats` has a `size` keyword argument to allow\n",
    "us to draw iid samples easily, with a small modification in Code Block\n",
    "[prior_batch](prior_batch) we have:\n",
    "\n",
    "```{code-block} python\n",
    ":name: prior_batch\n",
    ":caption: prior_batch\n",
    "\n",
    "def prior_sample(size):\n",
    "    x = stats.norm(loc=1.0, scale=2.0).rvs(size=size)\n",
    "    z = stats.halfnorm(loc=0., scale=1.).rvs(size=size)\n",
    "    y = stats.norm(loc=x, scale=z).rvs()\n",
    "    return x, z, y\n",
    "\n",
    "print([x.shape for x in prior_sample(size=(2))])\n",
    "print([x.shape for x in prior_sample(size=(2, 3, 5))])\n",
    "```\n",
    "\n",
    "```none\n",
    "[(2,), (2,), (2,)]\n",
    "[(2, 3, 5), (2, 3, 5), (2, 3, 5)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2,), (2,), (2,)]\n",
      "[(2, 3, 5), (2, 3, 5), (2, 3, 5)]\n"
     ]
    }
   ],
   "source": [
    "def prior_sample(size):\n",
    "    x = stats.norm(loc=1.0, scale=2.0).rvs(size=size)\n",
    "    z = stats.halfnorm(loc=0., scale=1.).rvs(size=size)\n",
    "    y = stats.norm(loc=x, scale=z).rvs()\n",
    "    return x, z, y\n",
    "\n",
    "print([x.shape for x in prior_sample(size=(2))])\n",
    "print([x.shape for x in prior_sample(size=(2, 3, 5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the function can handle arbitrary sample shape by adding\n",
    "`size` keyword argument when calling the random method `rvs`. Note\n",
    "however, for random variable `y`, we do not supply the `size` keyword\n",
    "argument as the sample shape is already implied from its parents.\n",
    "\n",
    "Consider another example in Code Block\n",
    "[prior_lm_batch](prior_lm_batch) for a linear regression\n",
    "model, we implemented `lm_prior_sample0` to draw one set of prior\n",
    "samples, and `lm_prior_sample` to draw a batch of prior samples.\n",
    "\n",
    "```{code-block} python\n",
    ":name: prior_lm_batch\n",
    ":caption: prior_lm_batch\n",
    "\n",
    "n_row, n_feature = 1000, 5\n",
    "X = np.random.randn(n_row, n_feature)\n",
    "\n",
    "def lm_prior_sample0():\n",
    "    intercept = stats.norm(loc=0, scale=10.0).rvs()\n",
    "    beta = stats.norm(loc=np.zeros(n_feature), scale=10.0).rvs()\n",
    "    sigma = stats.halfnorm(loc=0., scale=1.).rvs()\n",
    "    y_hat = X @ beta + intercept\n",
    "    y = stats.norm(loc=y_hat, scale=sigma).rvs()\n",
    "    return intercept, beta, sigma, y\n",
    "\n",
    "def lm_prior_sample(size=10):\n",
    "    if isinstance(size, int):\n",
    "        size = (size,)\n",
    "    else:\n",
    "        size = tuple(size)\n",
    "    intercept = stats.norm(loc=0, scale=10.0).rvs(size=size)\n",
    "    beta = stats.norm(loc=np.zeros(n_feature), scale=10.0).rvs(\n",
    "        size=size + (n_feature,))\n",
    "    sigma = stats.halfnorm(loc=0., scale=1.).rvs(size=size)\n",
    "    y_hat = np.squeeze(X @ beta[..., None]) + intercept[..., None]\n",
    "    y = stats.norm(loc=y_hat, scale=sigma[..., None]).rvs()\n",
    "    return intercept, beta, sigma, y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row, n_feature = 1000, 5\n",
    "X = np.random.randn(n_row, n_feature)\n",
    "\n",
    "def lm_prior_sample0():\n",
    "    intercept = stats.norm(loc=0, scale=10.0).rvs()\n",
    "    beta = stats.norm(loc=np.zeros(n_feature), scale=10.0).rvs()\n",
    "    sigma = stats.halfnorm(loc=0., scale=1.).rvs()\n",
    "    y_hat = X @ beta + intercept\n",
    "    y = stats.norm(loc=y_hat, scale=sigma).rvs()\n",
    "    return intercept, beta, sigma, y\n",
    "\n",
    "def lm_prior_sample(size=10):\n",
    "    if isinstance(size, int):\n",
    "        size = (size,)\n",
    "    else:\n",
    "        size = tuple(size)\n",
    "    intercept = stats.norm(loc=0, scale=10.0).rvs(size=size)\n",
    "    beta = stats.norm(loc=np.zeros(n_feature), scale=10.0).rvs(\n",
    "        size=size + (n_feature,))\n",
    "    sigma = stats.halfnorm(loc=0., scale=1.).rvs(size=size)\n",
    "    y_hat = np.squeeze(X @ beta[..., None]) + intercept[..., None]\n",
    "    y = stats.norm(loc=y_hat, scale=sigma[..., None]).rvs()\n",
    "    return intercept, beta, sigma, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(), (5,), (), (1000,)]\n"
     ]
    }
   ],
   "source": [
    "print([x.shape for x in lm_prior_sample0()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(), (5,), (), (1000,)]\n",
      "[(10,), (10, 5), (10,), (10, 1000)]\n",
      "[(10, 3), (10, 3, 5), (10, 3), (10, 3, 1000)]\n"
     ]
    }
   ],
   "source": [
    "print([x.shape for x in lm_prior_sample(size=())])\n",
    "print([x.shape for x in lm_prior_sample(size=10)])\n",
    "print([x.shape for x in lm_prior_sample(size=[10, 3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lm_prior_sample2(size=10):\n",
    "#     intercept = stats.norm(loc=0, scale=10.0).rvs(size=size)\n",
    "#     beta = stats.multivariate_normal(\n",
    "#         mean=np.zeros(n_feature), cov=10.0).rvs(size=size)\n",
    "#     sigma = stats.halfnorm(loc=0., scale=1.).rvs(size=size)\n",
    "#     y_hat = np.einsum('ij,...j->...i', X, beta) + intercept[..., None]\n",
    "#     y = stats.norm(loc=y_hat, scale=sigma[..., None]).rvs()\n",
    "#     return intercept, beta, sigma, y\n",
    "\n",
    "# print([x.shape for x in lm_prior_sample2(size=())])\n",
    "# print([x.shape for x in lm_prior_sample2(size=10)])\n",
    "# print([x.shape for x in lm_prior_sample2(size=(10, 3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two functions above, we see that to make the prior sample\n",
    "function to handle arbitrary sample shape, we need to make a few changes\n",
    "in `lm_prior_sample`:\n",
    "\n",
    "-   Supply `size` keyword argument to the sample call of root random\n",
    "    variables only;\n",
    "\n",
    "-   Supply `size + (n_feature,)` keyword argument to the sample call of\n",
    "    `beta` due to API limitations, which is a length `n_feature` vector\n",
    "    of regression coefficient. We need to additionally make sure `size`\n",
    "    is a tuple in the function so that it could be combined with the\n",
    "    original shape of `beta`;\n",
    "\n",
    "-   Shape handling by appending a dimension to `beta`, `intercept`, and\n",
    "    `sigma`, and squeezing of the matrix multiplication result so that\n",
    "    they are broadcast-able.\n",
    "\n",
    "As you can see, there is a lot of room for error and flexibility of how\n",
    "you might go about to implementing a \"shape-safe\\\" prior sample\n",
    "function. The complexity does not stop here, shape issues also pop up\n",
    "when computing model log probability and during inference (e.g., how\n",
    "non-scalar sampler MCMC kernel parameters broadcast to model\n",
    "parameters). There are convenience function transformations that\n",
    "vectorize your Python function such as `numpy.vectorize` or `jax.vmap`\n",
    "in JAX, but they are often not a silver bullet solution to fixing the\n",
    "all issues. For example, it requires additional user input if the\n",
    "vectorization is across multiple axes.\n",
    "\n",
    "An example of a well defined shape handling logic is the shape semantic\n",
    "in TensorFlow Probability {cite:p}`dillon2017tensorflow` [^28], which\n",
    "conceptually partitions a Tensor's shape into three groups:\n",
    "\n",
    "-   *Sample shape* that describes iid draws from the distribution.\n",
    "\n",
    "-   *Batch shape* that describes independent, not identically\n",
    "    distributed draws. Usually it is a set of (different)\n",
    "    parameterizations to the same distribution.\n",
    "\n",
    "-   *Event shape* that describes the shape of a single draw (event\n",
    "    space) from the distribution. For example, samples from multivariate\n",
    "    distributions have non-scalar event shape.\n",
    "\n",
    "Explicit batch shape is a powerful concept in TFP, which can be\n",
    "considered roughly along the line of *independent copy of the same thing\n",
    "that I would like to \"parallelly\\\" evaluate over*. For example,\n",
    "different chains from a MCMC trace, a batch of observation in mini-batch\n",
    "training, etc. For example, applying the shape semantic to the prior\n",
    "sample function in Code Block\n",
    "[prior_lm_batch](prior_lm_batch), we have a `beta`\n",
    "distributed as a `n_feature` batch of $\\mathcal{N}(0, 10)$ distribution.\n",
    "Note that while it is fine for the purpose of prior sampling, to be more\n",
    "precise we actually want the Event shape being `n_feature` instead of\n",
    "the batch shape. In that case the shape is correct for both forward\n",
    "random sampling and inverse log-probability computation. In NumPy it\n",
    "could be done by defining and sampling from a\n",
    "`stats.multivariate_normal` instead.\n",
    "\n",
    "When a user defines a TFP distribution, they can inspect the batch shape\n",
    "and the event shape to make sure it is working as intended. It is\n",
    "especially useful when writing a Bayesian model using\n",
    "`tfd.JointDistribution`. For example, we rewrite the regression model in\n",
    "Code Block [prior_lm_batch](prior_lm_batch) into Code\n",
    "Block [jd_lm_batch](jd_lm_batch) using\n",
    "`tfd.JointDistributionSequential`:\n",
    "\n",
    "```{code-block} python\n",
    ":name: jd_lm_batch\n",
    ":caption: jd_lm_batch\n",
    ":linenos:\n",
    "\n",
    "jd = tfd.JointDistributionSequential([\n",
    "    tfd.Normal(0, 10),\n",
    "    tfd.Sample(tfd.Normal(0, 10), n_feature),\n",
    "    tfd.HalfNormal(1),\n",
    "    lambda sigma, beta, intercept: tfd.Independent(\n",
    "        tfd.Normal(\n",
    "            loc=tf.einsum(\"ij,...j->...i\", X, beta) + intercept[..., None],\n",
    "            scale=sigma[..., None]),\n",
    "        reinterpreted_batch_ndims=1,\n",
    "        name=\"y\")\n",
    "])\n",
    "\n",
    "print(jd)\n",
    "\n",
    "n_sample = [3, 2]\n",
    "for log_prob_part in jd.log_prob_parts(jd.sample(n_sample)):\n",
    "    assert log_prob_part.shape == n_sample\n",
    "```\n",
    "\n",
    "```none\n",
    "tfp.distributions.JointDistributionSequential 'JointDistributionSequential' \n",
    "batch_shape=[[], [], [], []] \n",
    "event_shape=[[], [5], [], [1000]] \n",
    "dtype=[float32, float32, float32, float32]\n",
    "```\n",
    "\n",
    "A key thing to look for when ensuring the model is specified correctly\n",
    "is that `batch_shape` are consistent across arrays. In our example they\n",
    "are since they are all empty. Another helpful way to check that output\n",
    "is a structure of Tensor with the same shape `k` when calling\n",
    "`jd.log_prob_parts(jd.sample(k))` (line 15-17 in Code Block\n",
    "[jd_lm_batch](jd_lm_batch)). This will make sure the\n",
    "computation of the model log probability (e.g., for posterior inference)\n",
    "is correct. You can find a nice summary and visual demonstration of the\n",
    "shape semantic in TFP in a blog post by Eric J. Ma (*Reasoning about\n",
    "Shapes and Probability Distributions*) [^29].\n",
    "\n",
    "[^28]: See also\n",
    "    <https://www.tensorflow.org/probability/examples/TensorFlow_Distributions_Tutorial>\n",
    "\n",
    "[^29]: See\n",
    "    <https://ericmjl.github.io/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/>.\n",
    "    Luciano Paz also wrote an excellent introduction on shape handling\n",
    "    in PPLs in *PyMC3 shape handling*\n",
    "    <https://lucianopaz.github.io/2019/08/19/pymc3-shape-handling/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.JointDistributionSequential(\"JointDistributionSequential\", batch_shape=[[], [], [], []], event_shape=[[], [5], [], [1000]], dtype=[float32, float32, float32, float32])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "jd = tfd.JointDistributionSequential([\n",
    "    tfd.Normal(0, 10),\n",
    "    tfd.Sample(tfd.Normal(0, 10), n_feature),\n",
    "    tfd.HalfNormal(1),\n",
    "    lambda sigma, beta, intercept: tfd.Independent(\n",
    "        tfd.Normal(\n",
    "            loc=tf.einsum(\"ij,...j->...i\", X, beta) + intercept[..., None],\n",
    "            scale=sigma[..., None]),\n",
    "        reinterpreted_batch_ndims=1,\n",
    "        name=\"y\")\n",
    "])\n",
    "\n",
    "print(jd)\n",
    "\n",
    "n_sample = [3, 2]\n",
    "for log_prob_part in jd.log_prob_parts(jd.sample(n_sample)):\n",
    "    assert log_prob_part.shape == n_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(takeaways-for-the-applied-bayesian-practitioner)=\n",
    "\n",
    "## 10.9. Takeaways for the Applied Bayesian Practitioner\n",
    "\n",
    "We want to stress to the reader that the goal of this chapter is *not*\n",
    "to make you a proficient PPL designer but more so an informed PPL user.\n",
    "As a user, particularly if you are just starting out, it can be\n",
    "difficult to understand which PPL to choose and why. When you first\n",
    "learn about a PPL, it is good to keep in mind the basic components we\n",
    "listed in this chapter. For example, what primitives parameterize a\n",
    "distribution, how to evaluate the log-probability of some value, or what\n",
    "primitives define a random variables and how to link different random\n",
    "variables to construct a graphical model (the effect handling) etc.\n",
    "\n",
    "There are many considerations when picking PPL aside from the PPL\n",
    "itself. Given everything we have discussed in this chapter so far its\n",
    "very easy to get lost trying to optimize over each component to \"pick\n",
    "the best one\\\". Its also very easy for experienced practitioners to\n",
    "argue about why one PPL is better than another ad nauseum. Our advice is\n",
    "to pick the PPL that you feel most comfortable starting with and learn\n",
    "what is needed in your situation from applied experience.\n",
    "\n",
    "However, over time you will get a sense of what you need from a PPL and\n",
    "more importantly what you do not. We suggest trying out a couple of\n",
    "PPLs, in addition to the ones presented in this book, to get a sense of\n",
    "what will work for you. As a user you have the most to gain from\n",
    "actually *using* the PPL.\n",
    "\n",
    "Like Bayesian modeling when you explore the distribution of\n",
    "possibilities the collection of data becomes more informative than any\n",
    "single point. With the knowledge of how PPLs are constructed from this\n",
    "chapter, and personal experience through \"taking some for a spin\\\" we\n",
    "hope you will be finding the one that works best for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(exercises10)=\n",
    "\n",
    "## 10.10. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10E1.** Find a PPLs that utilizes another base language\n",
    "other than Python. Determine what differences are between PyMC3 or TFP.\n",
    "Specifically note a difference between the API and the computational\n",
    "backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10E2.** In this book we primarily use the PyData ecosystem.\n",
    "R is another popular programming language with a similar ecosystem. Find\n",
    "the R equivalents for\n",
    "\n",
    "-   Matplotlib\n",
    "\n",
    "-   The ArviZ LOO function\n",
    "\n",
    "-   Bayesian visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10E3.** What are other transformations that we have used on\n",
    "data and models throughout this book? What effect did they have? Hint:\n",
    "Refer to Chapter [3]](chap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10E4.** Draw a block diagram of a PPL[^30]. Label each\n",
    "component and explain what it does in your own words. There is no one\n",
    "right answer for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10E5.** Explain what batch shape, event shape, and sample\n",
    "shape are in your own words. In particular be sure to detail why its\n",
    "helpful to have each concept in a PPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10E6.** Find the Eight Schools NumPyro example online.\n",
    "Compare this to the TFP example, in particular noting the difference in\n",
    "primitives and syntax. What is similar? What is different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10E7.** Specify the following computation in Theano\n",
    "```{math}\n",
    ":label: eq:theano_comp\n",
    "\\sin(\\frac{1}{2}\\pi x) + \\exp(\\log(x)) + \\frac{(x-2)^2}{(x^2-4x+4)}\n",
    "```\n",
    "\n",
    "Generate the unoptimized computational graph. How many lines are\n",
    "printed. Use the `theano.function` method to run the optimizer. What is\n",
    "different about the optimized graph? Run a calculation using the\n",
    "optimized Theano function where $x=1$. What is the output value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10M8.** Create a model with following distributions using\n",
    "PyMC3.\n",
    "\n",
    "-   Gamma(alpha=1, beta=1)\n",
    "\n",
    "-   Binomial(p=5,12)\n",
    "\n",
    "-   TruncatedNormal(mu=2, sd=1, lower=1)\n",
    "\n",
    "Verify which distributions are automatically transformed from bounded to\n",
    "unbounded distributions. Plot samples from the priors for both the\n",
    "bounded priors and their paired transforms if one exists. What\n",
    "differences can you note?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10H9.** BlackJAX is a library of samplers for JAX. Generate\n",
    "a random sample of from $\\mathcal(N)(0,10)$ of size 20. Use the HMC\n",
    "sampler in JAX to recover the parameters of the data generating\n",
    "distribution. The BlackJAX documentation and Section {ref}`hmc`\n",
    "will be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10H10.** Implement the linear penguins model defined in\n",
    "Code Block\n",
    "[non_centered_regression](non_centered_regression) in\n",
    "NumPyro. After verifying the result are roughly the same as TFP and\n",
    "PyMC3, what differences do you see from the TFP and PyMC3 syntax? What\n",
    "similarities do you see? Be sure not just compare models, but compare\n",
    "the entire workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10H11.** We have explained reparameterization in previous\n",
    "chapter, for example, center and non-center parameterization for linear\n",
    "model in Section {ref}`model_geometry`. One of the use case\n",
    "for effect handling is to perform automatic reparameterization \n",
    "{cite:p}`gorinova2019automatic`. Try to write a effect handler in NumPyro to\n",
    "perform automatic non-centering of a random variable in a model. Hint:\n",
    "NumPyro already provides this functionality with\n",
    "`numpyro.handlers.reparam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
